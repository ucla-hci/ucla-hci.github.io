team:
  - name: Xiang 'Anthony' Chen
    role: Associate Professor
    expertise: Human Factors in AI
    img: Anthony_n.jpg
    imgalt: Anthony_f.jpg
    url: https://hci.prof
  - name: Bruce Liu
    role: ECE PhD
    expertise: HCI x AI
    img: Bruce_n.jpg
    imgalt: Bruce_f.jpg
    url: https://liubruce.me/
  - name: Youngseung Jeon
    role: ECE PhD
    expertise: HCI for Science
    img: Jay_n.jpg
    imgalt: Jay_f.jpg
    url: https://sites.google.com/view/youngseung-jeon
  - name: Christopher Hwang
    role: ECE MS
    expertise: UX Prototyper
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://www.linkedin.com/in/christopher-hwang-8b2894190/
  - name: Ziwen Li
    role: ECE MS
    expertise: Human-Centered NLP
    img: Aaron_n.png
    imgalt: Aaron_f.png
    url: https://aaronli43.github.io/personalweb/
  - name: WenFan Wang
    role: Visiting Scholar
    expertise: AI Creativity Support
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://vannpacks.github.io/ 
  - name: Serena Chang
    role: Undergrad Researcher
    expertise: UX Research
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://www.linkedin.com/in/serena-chang-878005245/
  - name: Tara Kaviani
    role: Undergrad Researcher
    expertise: Human-Centered AI
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://www.linkedin.com/in/tara-kaviani-040ba7245/
  - name: Zihan Zhou
    role: Visiting Scholar
    expertise: Visualization
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://scholar.google.com/citations?user=YPmvFYMAAAAJ&hl=zh-CN
  - name: Tengyou Xu
    role: ECE MS
    expertise: Medical AI
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Boyuan Jia
    role: Visiting Scholar
    expertise: Human-Centered AI
    img: minion.jpg
    imgalt: purple.jpeg
alumni:
  - name: 🎓 Dr. Hongyan Gu
    role: Senior Scientist at KUMC
    expertise: Human-Centered Medical AI
    img: Hongyan_n.jpg
    imgalt: Hongyan_f.jpg
    url: https://www.hygu.net/
  - name: Qinyi Zhou
    role: Visiting Scholar
    expertise: Human-AI Interaction
    img: Qinyi-s.jpg
    imgalt: Qinyi-f.jpg
    url: https://qinyizhou.notion.site/d1b006a021b048eeaa8ed580ca938f13?v=21de43e9d8da4db8b839583157d93092
  - name: Yujun Ge
    role: Highschool Scholar
    expertise: Human-Centered AI
    img: Yujun_s.png
    imgalt: Yujun_f.png
  - name: Philip Huang
    role: Undergrad Researcher
    expertise: Generative AI
    img: minion.jpg
    imgalt: purple.jpeg
    url: https://www.linkedin.com/in/philip-huang-15867b21b
  - name: 🎓 Dr. Jiahao Li
    role: HCI Researcher @ Apple
    expertise: Human-AI Interaction
    img: Nick-s.JPG
    imgalt: Nick-f.JPG
    url: https://ljhnick.github.io/
  - name: 🎓 Dr. Ruolin Wang
    role: Postdoc @ Georgia Tech
    expertise: Accessibility
    img: Ruolin-s.JPG
    imgalt: Ruolin-f.JPG
    url: https://www.violynnewang.com/
  - name: 🎓 Dr. Noyan Evirgen
    role: Data Scientist @ Spotter
    expertise: Generative AI
    img: Noyan_n.jpg
    imgalt: Noyan_f.jpg
  - name: Chunxu Yang
    role: Now PhD @ Waterloo
    expertise: Human-AI Interaction
    img: Chunxu_n.jpg
    imgalt: Chunxu_f.jpg
  - name: Naoto Nishida
    role: Visiting Researcher
    expertise: University of Tokyo
    img: Naoto_n.jpg
    imgalt: Naoto_f.jpg
    url: https://nawta.github.io/
  - name: Roy Jara
    role: ECE MS
    expertise: UX Prototyper
    img: Roy_n.jpg
    imgalt: Roy_f.jpg
    url: https://www.linkedin.com/in/roy-jara/
  - name: Wayne Zhang
    role: ECE MS
    expertise: UX Prototyper
    img: Wayne_n.jpg
    imgalt: Wayne_f.jpg
    url: https://www.linkedin.com/in/waynezhangus/
  - name: David Xiong
    role: CS Undergrad
    expertise: UX Engineer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Zihan Yan
    role: Collaborator
    expertise: Now PhD @ UIUC
    img: Zihan_n.jpg
    imgalt: Zihan_f.jpg
    url: https://yzihan.github.io/
  - name: Mina Huh
    role: Research Intern
    expertise: Now PhD @ UT Austin
    img: Mina-s.jpeg
    imgalt: Mina-f.jpeg
  - name: Yuan Liang
    role: ECE PhD
    expertise: Medical Imaging
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Yudai Tanaka
    role: Specialist
    expertise: Now PhD @ UChicago
    img: Yudai-s.JPG
    imgalt: Yudai-f.JPG
    url: https://yudai-tanaka.com
  - name: Lauren Hung
    role: Specialist
    expertise: Now MHCI @ CMU
    img: Lauren-s.JPG
    imgalt: Lauren-f.JPG
    url: http://laurenhung.com/
  - name: Shuen Chan
    role: Undergrad Researcher
    expertise: UX Designer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Julia Yin
    role: Undergrad Researcher
    expertise: UX Designer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Alexander Chen
    role: Undergrad Researcher
    expertise: <br>
    img: Alex-s.jpg
    imgalt: Alex-f.jpg
  - name: Alexiy Samoylov
    role: <a href='https://www.nsf.gov/awardsearch/showAward?AWD_ID=1850183' target='_blank'>NSF REU</a> Student
    expertise: Robotic Researcher
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Ashley Ngo
    role: Undergrad Researcher
    expertise: User Researcher
    img: Ashley-s.jpg
    imgalt: Ashley-f.jpg
  - name: Ritam Sarmah
    role: CS MS
    expertise: UX Architect
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Zixuan Chen
    role: Undergrad Researcher
    expertise: Web Engineer
    img: Zixuan-s.jpg
    imgalt: Zixuan-f.jpg
  - name: Claire Guo
    role: Undergrad Researcher
    expertise: User Researcher
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Yifan Xu
    role: ECE MS
    expertise: UX Engineer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Charisa Shin
    role: Undergrad Researcher
    expertise: UX Designer
    img: Charisa-s.jpg
    imgalt: Charisa-f.jpg
    url: http://charisa.design/
  - name: Amirali Omidfar
    role: ECE MS
    expertise: <br>
    img: Amir-s.JPG
    imgalt: Amir-f.JPG
  - name: Rita Dang
    role: Undergrad Researcher
    expertise: User Researcher
    img: Rita-s.jpg
    imgalt: Rita-f.jpg
  - name: Yunpeng Ding
    role: ECE MS
    expertise: NLP Engineer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Brandon Ngo
    role: Undergrad Researcher
    expertise: UX Designer
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Eric Perez
    role: Undergrad Researcher   
    expertise: Now MS at Cornell
    img: minion.jpg
    imgalt: purple.jpeg
  - name: James King
    role: Undergrad Researcher
    expertise: <br>
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Grace Zhao
    role: CS Undergrad
    expertise: <br>
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Sam Arlin
    role: CS PhD
    expertise: Interaction Techniques
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Carlo Rebanal
    role: Now ML Engineer at Oracle
    expertise: <br/>
    img: Carlo-s.JPG
    imgalt: Carlo-f.JPG
  - name: Yao Xie
    role: ECE MS
    expertise: <br/>
    img: Yao-s.JPG
    imgalt: Yao-f.JPG
  - name: Ximeng Liu
    role: ECE MS
    expertise: <br/>
    img: Simon-s.JPG
    imgalt: Simon-f.JPG
  - name: Nicolas Cheng
    role: ECE MS
    expertise: <br/>
    img: Nicky-s.JPG
    imgalt: Nicky-f.JPG
  - name: Yuki Tang
    role: Now MSc at UTokyo
    expertise: <br/>
    img: Yuki-s.JPG
    imgalt: Yuki-f.JPG 
  - name: Hsuan-Wei Fan
    role: Now MSc at UW
    expertise: <br/>
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Jingbin Huang
    role: <a href='https://www.nsf.gov/awardsearch/showAward?AWD_ID=1850183' target='_blank'>NSF REU</a> Student<br>Now MSc at UCSD
    img: Jingbin-s.JPG
    imgalt: Jingbin-f.JPG
  - name: Melody Chen
    role: <a href='https://www.nsf.gov/awardsearch/showAward?AWD_ID=1850183' target='_blank'>NSF REU</a> Student
    expertise: <br/>
    img: Melody-s.JPG
    imgalt: Melody-f.JPG 
  - name: Cathy Wang
    role: Now MSc at UCSD
    expertise: <br/>
    img: minion.jpg
    imgalt: purple.jpeg
  - name: Rohit Banerjee
    role: Now SoC Engineer at Intel
    expertise: <br/>
    img: Rohit-s.jpg
    imgalt: Rohit-f.jpg
  - name: Ben Wagstaff
    role: ECE Undergrad
    expertise: <br/>
    img: Ben-s.JPG
    imgalt: Ben-f.JPG
  - name: Joseph Lu
    role: CS Undergrad
    expertise: <br/>
    img: Joseph-s.JPG
    imgalt: Joseph-f.JPG
projects:
  - name: DANNY
    title: "Empowering Medical Data Labeling for Non-Experts with DANNY: Enhancing Accuracy and Mitigating Over-Reliance on AI"
    authors:
      - Youngseung Jeon, UCLA HCI Research
      - Christopher Hwang, UCLA HCI Research
      - Xiang ‘Anthony’ Chen, UCLA HCI Research
    pubs: IUI 2025
    img: t_danny.jpeg
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720326121657" title="2025_DANNY"><img src="https://live.staticflickr.com/65535/54520746405_392798958d_b.jpg" width="1024" height="768" alt="2025_DANNY"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    abstract: "Economic constraints on recruiting experts hinder efforts to build qualified datasets for utilizing AI in professional domains (e.g., medical diagnosis), which could provide societal benefits. To solve this issue, previous studies introduced crowdsourcing and AI to enable non-experts to perform expert-level data labeling. Yet, they encountered three challenges: 1) the limited applicability of crowdsourcing in less specialized domains (e.g., identifying animal species); 2) the chicken-and-egg problem, a paradox where high-performance AI is required to build a dataset to train such AI; and 3) over-reliance on AI, where non-experts, lacking expertise, may incorrectly label data when guided by sub-optimal AI. To address this, we introduce DANNY (Data ANnotation for Non-experts made easY), an AI-based tool designed to help non-experts label an arthritis dataset, aiming to increase labeling accuracy and mitigate over-reliance on AI. By externalizing a cognitive forcing intervention to foster critical thinking, DANNY provides two visualizations: 1) the Criteria phase, where non-experts define criteria across four arthritis features, and 2) the Correction phase, where they refine these criteria by comparing them to AI suggestions. In a study with 28 participants, DANNY users achieved higher accuracy and a more appropriate reliance on AI dependency than control groups. A follow-up study with 12 participants demonstrates how DANNY can be used to improve AI with an ensemble method. Our findings contribute new insights into using AI to support non-experts in labeling domain-specific data when expert resources are limited. "
    videoSite: vimeo
    video: '1084493767'
    bibtex: "@inproceedings{jeon2025empowering,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Empowering Medical Data Labeling for Non-Experts with DANNY: Enhancing Accuracy and Mitigating Over-Reliance on AI},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Jeon, Youngseung and Hwang, Christopher and Chen, XiangAnthony},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 30th International Conference on Intelligent User Interfaces},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={624--640},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2025}<br>}"
    citation: "Youngseung Jeon, Christopher Hwang, and Xiang 'Anthony' Chen. 2025. Empowering Medical Data Labeling for Non-Experts with DANNY: Enhancing Accuracy and Mitigating Over-Reliance on AI. In Proceedings of the 30th International Conference on Intelligent User Interfaces (IUI '25). Association for Computing Machinery, New York, NY, USA, 624–640. https://doi.org/10.1145/3708359.3712161"
    thumbnail: iui2025_danny_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1r1uquVX376pPk65PTaP9P2Iw3l_j68Zz/view?usp=sharing
  - name: QAPath
    title: "Majority Voting of Doctors Improves Appropriateness of AI Reliance in Pathology"
    authors:
      - Hongyan Gu, UCLA HCI Research
      - Chunxu Yang, UCLA HCI Research
      - Shino Magaki, UCLA David Geffen School of Medicine
      - Neda Zarrin-Khameh, Baylor College of Medicine
      - Nelli S. Lakis, University of Kansas Medical Center
      - Imna Cobos, Stanford School of Medicine
      - Negar Khanlou, UCLA David Geffen School of Medicine
      - Xinhai R. Zhang, University of Texas Health Science Center at Houston
      - Jasmeet Assi, University of Kansas Medical Center
      - Joshua T. Byers, University of California, San Francisco
      - Ameer Hamza, University of Kansas Medical Center
      - Karam Han, University of Wisconsin-Madison 
      - Anders Meyer, University of Kansas Medical Center
      - Hilda Mirbaha, UCLA David Geffen School of Medicine
      - Carrie A. Mohila, Baylor College of Medicine
      - Todd M. Stevens, University of Kansas Medical Center
      - Sara L. Stone, Hospital of the University of Pennsylvania
      - Wenzhong Yan, UCLA ECE
      - Mohammad Haeri, University of Kansas Medical Center
      - Xiang ‘Anthony’ Chen, UCLA HCI Research
    pubs: IJHCS 2024
    img: t_qapath.jpg
    abstract: "As Artificial Intelligence (AI) making advancements in medical decision-making, there is a growing need to ensure doctors develop appropriate reliance on AI to avoid adverse outcomes. However, existing methods in enabling appropriate AI reliance might encounter challenges while being applied in the medical domain. With this regard, this work employs and provides the validation of an alternative approach – majority voting – to facilitate appropriate reliance on AI in medical decision-making. This is achieved by a multi-institutional user study involving 32 medical professionals with various backgrounds, focusing on the pathology task of visually detecting a pattern, mitoses, in tumor images. Here, the majority voting process was conducted by synthesizing decisions under AI assistance from a group of pathology doctors (pathologists). Two metrics were used to evaluate the appropriateness of AI reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR – by approximately 9% and 31%, respectively – compared to decisions made by one pathologist collaborating with AI. This increased appropriateness resulted in better precision and recall in the detection of mitoses. While our study is centered on pathology, we believe these insights can be extended to general high-stakes decision-making processes involving similar visual tasks."
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720326069466" title="2024_QAPath"><img src="https://live.staticflickr.com/65535/54516007859_ae8b131fc9_b.jpg" width="1024" height="768" alt="2024_QAPath"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Hongyan Gu, Chunxu Yang, Shino Magaki, Neda Zarrin-Khameh, Nelli S. Lakis, Inma Cobos, Negar Khanlou, Xinhai R. Zhang, Jasmeet Assi, Joshua T. Byers, Ameer Hamza, Karam Han, Anders Meyer, Hilda Mirbaha, Carrie A. Mohila, Todd M. Stevens, Sara L. Stone, Wenzhong Yan, Mohammad Haeri, Xiang ‘Anthony’ Chen, Majority voting of doctors improves appropriateness of AI reliance in pathology, International Journal of Human-Computer Studies, Volume 190, 2024, 103315, ISSN 1071-5819, https://doi.org/10.1016/j.ijhcs.2024.103315."
    bibtex: "@article{GU2024103315,<br>title = {Majority voting of doctors improves appropriateness of AI reliance in pathology},<br>journal = {International Journal of Human-Computer Studies},<br>volume = {190},<br>pages = {103315},<br>year = {2024},<br>issn = {1071-5819},<br>doi = {https://doi.org/10.1016/j.ijhcs.2024.103315},<br>url = {https://www.sciencedirect.com/science/article/pii/S1071581924000995},<br>author = {Hongyan Gu and Chunxu Yang and Shino Magaki and Neda Zarrin-Khameh and Nelli S. Lakis and Inma Cobos and Negar Khanlou and Xinhai R. Zhang and Jasmeet Assi and Joshua T. Byers and Ameer Hamza and Karam Han and Anders Meyer and Hilda Mirbaha and Carrie A. Mohila and Todd M. Stevens and Sara L. Stone and Wenzhong Yan and Mohammad Haeri and Xiang ‘Anthony’ Chen},<br>keywords = {Appropriate reliance, Artificial intelligence, Majority voting, Pathology},<br>abstract = {As Artificial Intelligence (AI) making advancements in medical decision-making, there is a growing need to ensure doctors develop appropriate reliance on AI to avoid adverse outcomes. However, existing methods in enabling appropriate AI reliance might encounter challenges while being applied in the medical domain. With this regard, this work employs and provides the validation of an alternative approach – majority voting – to facilitate appropriate reliance on AI in medical decision-making. This is achieved by a multi-institutional user study involving 32 medical professionals with various backgrounds, focusing on the pathology task of visually detecting a pattern, mitoses, in tumor images. Here, the majority voting process was conducted by synthesizing decisions under AI assistance from a group of pathology doctors (pathologists). Two metrics were used to evaluate the appropriateness of AI reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR – by approximately 9% and 31%, respectively – compared to decisions made by one pathologist collaborating with AI. This increased appropriateness resulted in better precision and recall in the detection of mitoses. While our study is centered on pathology, we believe these insights can be extended to general high-stakes decision-making processes involving similar visual tasks.}<br>}"
    thumbnail: ijhcs_qapath_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1qsGZCHt03QqiKs_I5pDdxfhDa3dzQ-Ao/view?usp=sharing
  - name: "Human I/O"
    title: "Human I/O: Towards a Unified Approach to Detecting Situational Impairments"
    authors:
      - Xingyu Bruce Liu, UCLA HCI Research
      - Jiaohao Nick Li, UCLA HCI Research
      - David Kim, Google
      - Xiang 'Anthony' Chen, UCLA HCI Research
      - Ruofei Du, Google
    pubs: CHI 2024 🏅️
    img: t_human_io.png
    abstract: "Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future."
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720322425608" title="2023_Human_IO"><img src="https://live.staticflickr.com/65535/54188474215_5468b6854c_b.jpg" width="1024" height="768" alt="2023_Human_IO"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    videoSite: vimeo
    video: "1036940920"
    citation: "Xingyu Bruce Liu, Jiahao Nick Li, David Kim, Xiang 'Anthony' Chen, and Ruofei Du. 2024. Human I/O: Towards a Unified Approach to Detecting Situational Impairments. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 965, 1–18. https://doi.org/10.1145/3613904.3642065"
    bibtex: "@inproceedings{10.1145/3613904.3642065,<br>author = {Liu, Xingyu Bruce and Li, Jiahao Nick and Kim, David and Chen, Xiang 'Anthony' and Du, Ruofei},<br>title = {Human I/O: Towards a Unified Approach to Detecting Situational Impairments},<br>year = {2024},<br>isbn = {9798400703300},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3613904.3642065},<br>doi = {10.1145/3613904.3642065},<br>booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},<br>articleno = {965},<br>numpages = {18},<br>keywords = {augmented reality, context awareness, large language models, multimodal sensing, situational impairments},<br>location = {Honolulu, HI, USA},<br>series = {CHI '24}<br>}"
    thumbnail: pt_human_io.jpg
    paperUrl: https://drive.google.com/file/d/1Q1OizH5rffaz6R3ct8kT4NGeS-71b-rq/view?usp=sharing
  - name: "Meningioma Mitosis AI"
    title: Enhancing Mitosis Count Assessment in Meningiomas with Computational Digital Pathology
    authors:
      - Hongyan Gu, UCLA HCI Research
      - Chunxu Yang, UCLA HCI Research
      - Issa Al-kharouf, University of Kansas Medical Center
      - Shino Magaki, UCLA David Geffen School of Medicine
      - Nelli Lakis, University of Kansas Medical Center
      - Christopher Kazu Williams, UCLA David Geffen School of Medicine
      - Sallam Mohammad Alrosan, University of Kansas Medical Center
      - Ellie K. Onstott, University of Kansas Medical Center
      - Wenzhong Yan, UCLA ECE
      - Negar Khanlou, UCLA David Geffen School of Medicine
      - Imna Cobos, Stanford School of Medicine
      - Xinhai R. Zhang, University of Texas Health Science Center at Houston
      - Neda Zarrin-Khameh, Baylor College of Medicine
      - Harry V. Vinters, UCLA David Geffen School of Medicine
      - Xiang 'Anthony' Chen, UCLA HCI Research
      - Mohammad Haeri, University of Kansas Medical Center
    pubs: Acta Neuropathol Commun 2024
    img: t_mitosis_ai.jpg
    abstract: "Mitosis is a critical criterion for meningioma grading. However, pathologists’ assessment of mitoses is subject to significant inter-observer variation due to challenges in locating mitosis hotspots and accurately detecting mitotic figures. To address this issue, we leverage digital pathology and propose a computational strategy to enhance pathologists’ mitosis assessment. The strategy has two components: (1) A depth-first search algorithm that quantifies the mathematically maximum mitotic count in 10 consecutive high-power fields, which can enhance the preciseness, especially in cases with borderline mitotic count. (2) Implementing a collaborative sphere to group a set of pathologists to detect mitoses under each high-power field, which can mitigate subjective random errors in mitosis detection originating from individual detection errors. By depth-first search algorithm (1) , we analyzed 19 meningioma slides and discovered that the proposed algorithm upgraded two borderline cases verified at consensus conferences. This improvement is attributed to the algorithm’s ability to quantify the mitotic count more comprehensively compared to other conventional methods of counting mitoses. In implementing a collaborative sphere (2) , we evaluated the correctness of mitosis detection from grouped pathologists and/or pathology residents, where each member of the group annotated a set of 48 high-power field images for mitotic figures independently. We report that groups with sizes of three can achieve an average precision of 0.897 and sensitivity of 0.699 in mitosis detection, which is higher than an average pathologist in this study (precision: 0.750, sensitivity: 0.667). The proposed computational strategy can be integrated with artificial intelligence workflow, which envisions the future of achieving a rapid and robust mitosis assessment by interactive assisting algorithms that can ultimately benefit patient management."
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720319214247" title="2024_Mitosis_AI"><img src="https://live.staticflickr.com/65535/53890556708_330890a107_c.jpg" width="1024" height="768" alt="2024_Mitosis_AI"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    videoSite: vimeo
    video: "991874097"
    citation: "Gu H, Yang C, Al-Kharouf I, Magaki S, Lakis N, Williams CK, Alrosan SM, Onstott EK, Yan W, Khanlou N, Cobos I, Zhang XR, Zarrin-Khameh N, Vinters HV, Chen XA, Haeri M. Enhancing mitosis quantification and detection in meningiomas with computational digital pathology. Acta Neuropathol Commun. 2024 Jan 11;12(1):7. doi: 10.1186/s40478-023-01707-6. PMID: 38212848; PMCID: PMC10782692."
    bibtex: "@article{gu2024enhancing,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Enhancing mitosis quantification and detection in meningiomas with computational digital pathology},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Gu, Hongyan and Yang, Chunxu and Al-Kharouf, Issa and Magaki, Shino and Lakis, Nelli and Williams, Christopher Kazu and Alrosan, Sallam Mohammad and Onstott, Ellie Kate and Yan, Wenzhong and Khanlou, Negar and others},<br>&nbsp; &nbsp; &nbsp; &nbsp;journal={Acta Neuropathologica Communications},<br>&nbsp; &nbsp; &nbsp; &nbsp;volume={12},<br>&nbsp; &nbsp; &nbsp; &nbsp;number={1},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={7},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2024},<br>&nbsp; &nbsp; &nbsp; &nbsp;publisher={Springer}<br>}"
    thumbnail: pt_mitosis_ai.jpg
    paperUrl: https://drive.google.com/open?id=1vLroIHjiDya_4oWEiQPks6euQv-48Bx3
  - name: "XAI for Text-to-Image Generation"
    title: "From Text to Pixels: Enhancing User Understanding through Text-to-Image Model Explanations"
    authors:
      - Noyan Evirgen (UCLA HCI Research)
      - Ruolin Wang (UCLA HCI Research)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: IUI 2024
    img: t_txt2pix.png
    abstract: "Recent progress in Text-to-Image (T2I) models promises transformative applications in art, design, education, medicine, and entertainment. These models, exemplified by Dall-e, Imagen, and Stable Diffusion, have the potential to revolutionize various industries. However, a primary concern is their operation as a 'black-box' for many users. Without understanding the underlying mechanics, users are unable to harness the full potential of these models. This study focuses on bridging this gap by developing and evaluating explanation techniques for T2I models, targeting inexperienced end users. While prior works have delved into Explainable AI (XAI) methods for classification or regression tasks, T2I generation poses distinct challenges. Through formative studies with experts, we identified unique explanation goals and subsequently designed tailored explanation strategies. We then empirically evaluated these methods with a cohort of 473 participants from Amazon Mechanical Turk (AMT) across three tasks. Our results highlight users' ability to learn new keywords through explanations, a preference for example-based explanations, and challenges in comprehending explanations that significantly shift the image's theme. Moreover, findings suggest users benefit from a limited set of concurrent explanations. Our main contributions include a curated dataset for evaluating T2I explainability techniques, insights from a comprehensive AMT user study, and observations critical for future T2I model explainability research."
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720319101720" title="2024_XAI_txt2pix"><img src="https://live.staticflickr.com/65535/53879868409_f57485e068_b.jpg" width="1024" height="768" alt="2024_XAI_txt2pix"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Noyan Evirgen, Ruolin Wang, and Xiang 'Anthony Chen. 2024. From Text to Pixels: Enhancing User Understanding through Text-to-Image Model Explanations. In Proceedings of the 29th International Conference on Intelligent User Interfaces (IUI '24). Association for Computing Machinery, New York, NY, USA, 74–87. https://doi.org/10.1145/3640543.3645173"
    bibtex: "@inproceedings{10.1145/3640543.3645173,<br>author = {Evirgen, Noyan and Wang, Ruolin and Chen, Xiang 'Anthony},<br>title = {From Text to Pixels: Enhancing User Understanding through Text-to-Image Model Explanations},<br>year = {2024},<br>isbn = {9798400705083},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3640543.3645173},<br>doi = {10.1145/3640543.3645173},<br>abstract = {Recent progress in Text-to-Image (T2I) models promises transformative applications in art, design, education, medicine, and entertainment. These models, exemplified by Dall-e, Imagen, and Stable Diffusion, have the potential to revolutionize various industries. However, a primary concern is their operation as a ‘black-box’ for many users. Without understanding the underlying mechanics, users are unable to harness the full potential of these models. This study focuses on bridging this gap by developing and evaluating explanation techniques for T2I models, targeting inexperienced end users. While prior works have delved into Explainable AI (XAI) methods for classification or regression tasks, T2I generation poses distinct challenges. Through formative studies with experts, we identified unique explanation goals and subsequently designed tailored explanation strategies. We then empirically evaluated these methods with a cohort of 473 participants from Amazon Mechanical Turk (AMT) across three tasks. Our results highlight users’ ability to learn new keywords through explanations, a preference for example-based explanations, and challenges in comprehending explanations that significantly shift the image’s theme. Moreover, findings suggest users benefit from a limited set of concurrent explanations. Our main contributions include a curated dataset for evaluating T2I explainability techniques, insights from a comprehensive AMT user study, and observations critical for future T2I model explainability research.},<br>booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},<br>pages = {74–87},<br>numpages = {14},<br>keywords = {Explainability Methods, Text-to-Image, User-Study, XAI},<br>location = {Greenville, SC, USA},<br>series = {IUI '24}<br>}"
    thumbnail: pt_txt2pix.jpg
    paperUrl: https://drive.google.com/open?id=1KrRKs_ZF7G4addZo8WMX-VotDhbM97nl
  - name: "HCI Papers Cite HCI Papers"
    title: "HCI Papers Cite HCI Papers, Increasingly So"
    authors:
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: alt.chi 2024
    img: t_xindex.jpg
    abstract: "To measure how HCI papers are cited across disciplinary boundaries, we collected a citation dataset of CHI, UIST, and CSCW papers published between 2010 and 2020. Our analysis indicates that HCI papers have been more and more likely to be cited by HCI papers rather than by non-HCI papers."
    videoSite: vimeo
    video: "936893053"
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720316332160" title="2024_Xindex"><img src="https://live.staticflickr.com/65535/53665801593_717b34b91b_b.jpg" width="1024" height="768" alt="2024_Xindex"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: Xiang ‘Anthony’ Chen. HCI Papers Cite HCI Papers, Increasingly So. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems (CHI EA '24).
    bibtex: "@misc{chen2024hci,<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;title={HCI Papers Cite HCI Papers, Increasingly So}, <br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;author={Xiang Anthony Chen},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;year={2024},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;eprint={2303.07539},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;archivePrefix={arXiv},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;primaryClass={cs.HC}<br>}"
    thumbnail: altchi2024_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/16Ee2XWWAb8XuvO7EC3PH5BFJgIPp2bA8/view?usp=sharing
  - name: 'Marvista'
    title: 'Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool'
    authors:
      - Xiang 'Anthony' Chen (UCLA HCI Research)
      - Chien-Sheng Wu, Lidiya Murakhovs'ka, Philippe Laban, Tong Niu, Wenhao Liu, Caiming Xiong (Salesforce Research)
    pubs: TOCHI 2023
    img: t_marvista.jpeg
    abstract: "We explore the design of Marvista—a human-AI collaborative tool that employs a suite of natural language processing models to provide end-to-end support for reading online news articles. Before reading an article, Marvista helps a user plan what to read by filtering text based on how much time one can spend and what questions one is interested to find out from the article. During reading, Marvista helps the user reflect on their understanding of each paragraph with AI-generated questions. After reading, Marvista generates an explainable human-AI summary that combines both AI’s processing of the text, the user’s reading behavior, and user-generated data in the reading process. In contrast to prior work that offered (content-independent) interaction techniques or devices for reading, Marvista takes a human-AI collaborative approach that contributes text-specific guidance (content-aware) to support the entire reading process."
    videoSite: vimeo
    video: 858105039
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720310751724" title="2023_Marvista"><img src="https://live.staticflickr.com/65535/53142308850_1843b016f4_b.jpg" width="1024" height="768" alt="2023_Marvista"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Xiang ‘Anthony’ Chen, Chien-Sheng Wu, Lidiya Murakhovs’ka, Philippe Laban, Tong Niu, Wenhao Liu, and Caiming Xiong. 2023. Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool. ACM Trans. Comput.-Hum. Interact. Just Accepted (July 2023). https://doi.org/10.1145/3609331"
    bibtex: "@article{10.1145/3609331,<br>author = {Chen, Xiang ‘Anthony’ and Wu, Chien-Sheng and Murakhovs’ka, Lidiya and Laban, Philippe and Niu, Tong and Liu, Wenhao and Xiong, Caiming},<br>title = {Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool},<br>year = {2023},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>issn = {1073-0516},<br>url = {https://doi.org/10.1145/3609331},<br>doi = {10.1145/3609331},<br>abstract = {We explore the design of Marvista—a human-AI collaborative tool that employs a suite of natural language processing models to provide end-to-end support for reading online news articles. Before reading an article, Marvista helps a user plan what to read by filtering text based on how much time one can spend and what questions one is interested to find out from the article. During reading, Marvista helps the user reflect on their understanding of each paragraph with AI-generated questions. After reading, Marvista generates an explainable human-AI summary that combines both AI’s processing of the text, the user’s reading behavior, and user-generated data in the reading process. In contrast to prior work that offered (content-independent) interaction techniques or devices for reading, Marvista takes a human-AI collaborative approach that contributes text-specific guidance (content-aware) to support the entire reading process.},<br>note = {Just Accepted},<br>journal = {ACM Trans. Comput.-Hum. Interact.},<br>month = {jul},<br>keywords = {Tools, Human-AI Collaboration, Reading}<br>}"
    thumbnail: tochi2023_marvista_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1nL3U-R2HcSkF5we_j-CdcpDssLddZgSR/view?usp=sharing
  - name: 'Visual Captions'
    title: 'Visual Captions: Augmenting Verbal Communication with On-the-fly Visuals'
    authors:
      - Xingyu "Bruce" Liu (UCLA HCI Research)
      - Vladimir Kirilyuk, Xiuxiu Yuan, Alex Olwal, Peggy Chi (Google)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
      - Ruofei Du (Google)
    pubs: CHI 2023
    img: t_visualcaptions.png
    abstract: "Video conferencing solutions like Zoom, Google Meet, and Microsoft Teams are becoming increasingly popular for facilitating conversations, and recent advancements such as live captioning help people better understand each other. We believe that the addition of visuals based on the context of conversations could further improve comprehension of complex or unfamiliar concepts. To explore the potential of such capabilities, we conducted a formative study through remote interviews (N=10) and crowdsourced a dataset of over 1500 sentence-visual pairs across a wide range of contexts. These insights informed Visual Captions, a real-time system that integrates with a videoconferencing platform to enrich verbal communication. Visual Captions leverages a fine-tuned large language model to proactively suggest relevant visuals in open-vocabulary conversations. We present the findings from a lab study (N=26) and an in-the-wild case study (N=10), demonstrating how Visual Captions can help improve communication through visual augmentation in various scenarios."
    videoSite: vimeo
    video: 820034807
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720307685527" title="2023_VisualCaptions"><img src="https://live.staticflickr.com/65535/52835443692_b24b9dce47_b.jpg" width="1024" height="768" alt="2023_VisualCaptions"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Xingyu 'Bruce' Liu, Vladimir Kirilyuk, Xiuxiu Yuan, Alex Olwal, Peggy Chi, Xiang 'Anthony' Chen, and Ruofei Du. 2023. Visual Captions: Augmenting Verbal Communication with On-the-fly Visuals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 108, 1-20. https://doi.org/10.1145/3544548.3581566"
    bibtex: "@inproceedings{10.1145/3544548.3581566,<br>author = {Xingyu, Bruce”@ and Kirilyuk, Vladimir and Yuan, Xiuxiu and Olwal, Alex and Chi, Peggy and Chen, Xiang 'Anthony' and Du, Ruofei},<br>title = {Visual Captions: Augmenting Verbal Communication with On-the-Fly Visuals},<br>year = {2023},<br>isbn = {9781450394215},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3544548.3581566},<br>doi = {10.1145/3544548.3581566},<br>abstract = {Video conferencing solutions like Zoom, Google Meet, and Microsoft Teams are becoming increasingly popular for facilitating conversations, and recent advancements such as live captioning help people better understand each other. We believe that the addition of visuals based on the context of conversations could further improve comprehension of complex or unfamiliar concepts. To explore the potential of such capabilities, we conducted a formative study through remote interviews (N=10) and crowdsourced a dataset of over 1500 sentence-visual pairs across a wide range of contexts. These insights informed Visual Captions, a real-time system that integrates with a video conferencing platform to enrich verbal communication. Visual Captions leverages a fine-tuned large language model to proactively suggest relevant visuals in open-vocabulary conversations. We present findings from a lab study (N=26) and an in-the-wild case study (N=10), demonstrating how Visual Captions can help improve communication through visual augmentation in various scenarios.},<br>booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},<br>articleno = {108},<br>numpages = {20},<br>keywords = {large language models, augmented reality, video-mediated communication, augmented communication, AI agent, text-to-visual, online meeting, dataset, collaborative work},<br>location = {Hamburg, Germany},<br>series = {CHI '23}<br>}"
    thumbnail: chi2023_visualcaptions_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1d5n9Mlftl2uAa8OmAPkjilWWhk9Tsksi/view?usp=sharing
  - name: 'NaviPath'
    title: 'Augmenting Pathologists with NaviPath: Design and Evaluation of a Human-AI Collaborative Navigation System'
    authors:
      - Hongyan Gu, Chunxu Yang (UCLA HCI Research)
      - Mohammad Haeri (University of Kansas Medical Center)
      - Jing Wang, Beijing Tongren Hospital, Capital Medical University
      - Shirley Tang, Wenzhong Yan (UCLA HCI Research)
      - Shujin He, Beijing Tongren Hospital, Capital Medical University
      - Christopher Kazu Williams, Shino Magaki (UCLA David Geffen School of Medicine)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: CHI 2023 🏅️
    img: t_navipath.jpg
    abstract: "Artificial Intelligence (AI) brings advancements to support pathologists in navigating high-resolution tumor images to search for pathology patterns of interest. However, existing AI-assisted tools have not realized this promised potential due to a lack of insight into pathology and HCI considerations for pathologists' navigation workflows in practice. We first conducted a formative study with six medical professionals in pathology to capture their navigation strategies. By incorporating our observations along with the pathologists' domain knowledge, we designed NaviPath — a human-AI collaborative navigation system. An evaluation study with 15 medical professionals in pathology indicated that: (i) compared to the manual navigation, participants saw more than twice the number of pathological patterns in unit time with NaviPath, and (ii) participants achieved higher precision and recall against the AI and the manual navigation on average. Further qualitative analysis revealed that navigation was more consistent with NaviPath, which can improve the overall examination quality."
    videoSite: vimeo
    video: 820033630
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720307685222" title="2023_NaviPath"><img src="https://live.staticflickr.com/65535/52836157174_f01d853d39_b.jpg" width="1024" height="768" alt="2023_NaviPath"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Hongyan Gu, Chunxu Yang, Mohammad Haeri, Jing Wang, Shirley Tang, Wenzhong Yan, Shujin He, Christopher Kazu Williams, Shino Magaki, and Xiang 'Anthony' Chen. 2023. Augmenting Pathologists with NaviPath: Design and Evaluation of a Human-AI Collaborative Navigation System. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 349, 1-19. https://doi.org/10.1145/3544548.3580694"
    bibtex: "@inproceedings{10.1145/3544548.3580694,<br>author = {Gu, Hongyan and Yang, Chunxu and Haeri, Mohammad and Wang, Jing and Tang, Shirley and Yan, Wenzhong and He, Shujin and Williams, Christopher Kazu and Magaki, Shino and Chen, Xiang 'Anthony'},<br>title = {Augmenting Pathologists with NaviPath: Design and Evaluation of a Human-AI Collaborative Navigation System},<br>year = {2023},<br>isbn = {9781450394215},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3544548.3580694},<br>doi = {10.1145/3544548.3580694},<br>abstract = {Artificial Intelligence (AI) brings advancements to support pathologists in navigating high-resolution tumor images to search for pathology patterns of interest. However, existing AI-assisted tools have not realized this promised potential due to a lack of insight into pathology and HCI considerations for pathologists' navigation workflows in practice. We first conducted a formative study with six medical professionals in pathology to capture their navigation strategies. By incorporating our observations along with the pathologists' domain knowledge, we designed NaviPath&nbsp;— a human-AI collaborative navigation system. An evaluation study with 15 medical professionals in pathology indicated that: (i)&nbsp;compared to the manual navigation, participants saw more than twice the number of pathological patterns in unit time with NaviPath, and (ii)&nbsp;participants achieved higher precision and recall against the AI and the manual navigation on average. Further qualitative analysis revealed that navigation was more consistent with NaviPath, which can improve the overall examination quality.},<br>booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},<br>articleno = {349},<br>numpages = {19},<br>keywords = {medical AI, Human-AI collaboration, navigation, digital pathology},<br>location = {Hamburg, Germany},<br>series = {CHI '23}<br>}"
    thumbnail: chi2023_navipath_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1cpqUMO0jg5E3LAnwy9AUXW712Ipz3yvw/view?usp=sharing
  - name: 'GANravel'
    title: 'GANravel: User-Driven Direction Disentanglement in Generative Adversarial Networks'
    authors:
      - Noyan Evirgen (UCLA HCI Research)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: CHI 2023
    img: t_ganravel.png
    abstract: "Generative adversarial networks (GANs) have many application areas including image editing, domain translation, missing data imputation, and support for creative work. However, GANs are considered `black boxes'. Specifically, the end-users have little control over how to improve editing directions through disentanglement. Prior work focused on new GAN architectures to disentangle editing directions. Alternatively, we propose GANravel --a user-driven direction disentanglement tool that complements the existing GAN architectures and allows users to improve editing directions iteratively. In two user studies with 16 participants each, GANravel users were able to disentangle directions and outperformed the state-of-the-art direction discovery baselines in disentanglement performance. In the second user study, GANravel was used in a creative task of creating dog memes and was able to create high-quality edited images and GIFs."
    videoSite: vimeo
    video: 809403374
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720306814055" title="2023_GANravel"><img src="https://live.staticflickr.com/65535/52756416835_6cdac1cc04_b.jpg" width="1024" height="768" alt="2023_GANravel"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Noyan Evirgen and Xiang 'Anthony Chen. 2023. GANravel: User-Driven Direction Disentanglement in Generative Adversarial Networks. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 19, 1-15. https://doi.org/10.1145/3544548.3581226"
    bibtex: "@inproceedings{10.1145/3544548.3581226,<br>author = {Evirgen, Noyan and Chen, Xiang 'Anthony},<br>title = {GANravel: User-Driven Direction Disentanglement in Generative Adversarial Networks},<br>year = {2023},<br>isbn = {9781450394215},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3544548.3581226},<br>doi = {10.1145/3544548.3581226},<br>abstract = {Generative adversarial networks (GANs) have many application areas including image editing, domain translation, missing data imputation, and support for creative work. However, GANs are considered 'black boxes'. Specifically, the end-users have little control over how to improve editing directions through disentanglement. Prior work focused on new GAN architectures to disentangle editing directions. Alternatively, we propose GANravel—a user-driven direction disentanglement tool that complements the existing GAN architectures and allows users to improve editing directions iteratively. In two user studies with 16 participants each, GANravel users were able to disentangle directions and outperformed the state-of-the-art direction discovery baselines in disentanglement performance. In the second user study, GANravel was used in a creative task of creating dog memes and was able to create high-quality edited images and GIFs.},<br>booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},<br>articleno = {19},<br>numpages = {15},<br>keywords = {Disentanglement, Generative Adversarial Networks, Interactive Systems, Explainable-AI},<br>location = {Hamburg, Germany},<br>series = {CHI '23}<br>}"
    thumbnail: chi2023_ganravel_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1ccVWtz4ZtBCANm4R904E_0LOpuxEY3Iy/view?usp=sharing
  - name: 'xPath'
    title: 'Improving Workflow Integration with xPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology'
    authors:
      - Hongyan Gu, Yuan Liang, Yifan Xu, Chunxu Yang, Wenzhong Yan (UCLA HCI Research)
      - Christopher Kazu Williams, Shino Magaki, Negar Khanlou, Harry Vinters, Zesheng Chen (UCLA David Geffen School of Medicine)
      - Shuo Ni (UCLA and USC)
      - Xinhai Robert Zhang (University of Texas Health Science Center at Houston)
      - Yang Li (Google Research)
      - Mohammad Haeri (University of Kansas Medical Center)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: TOCHI 2022
    img: t_xpath.jpeg
    abstract: "Recent developments in AI have provided assisting tools to support pathologists' diagnoses. However, it remains challenging to incorporate such tools into pathologists' practice; one main concern is AI's insufficient workflow integration with medical decisions. We observed pathologists' examination and discovered that the main hindering factor to integrate AI is its incompatibility with pathologists' workflow. To bridge the gap between pathologists and AI, we developed a human-AI collaborative diagnosis tool — xPath — that shares a similar examination process to that of pathologists, which can improve AI's integration into their routine examination. The viability of xPath is confirmed by a technical evaluation and work sessions with twelve medical professionals in pathology. This work identifies and addresses the challenge of incorporating AI models into pathology, which can offer first-hand knowledge about how HCI researchers can work with medical professionals side-by-side to bring technological advances to medical tasks towards practical applications."
    videoSite: vimeo
    video: 789344703
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720305255518" title="2022_xPath"><img src="https://live.staticflickr.com/65535/52627677410_8bf94fa615_b.jpg" width="1024" height="768" alt="2022_xPath"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Hongyan Gu, Yuan Liang, Yifan Xu, Christopher Kazu Williams, Shino Magaki, Negar Khanlou, Harry Vinters, Zesheng Chen, Shuo Ni, Chunxu Yang, Wenzhong Yan, Xinhai Robert Zhang, Yang Li, Mohammad Haeri, and Xiang 'Anthony' Chen. 2022. Improving Workflow Integration with xPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology. ACM Trans. Comput.-Hum. Interact. Just Accepted (December 2022). https://doi.org/10.1145/3577011"
    bibtex: "@article{10.1145/3577011,<br>author = {Gu, Hongyan and Liang, Yuan and Xu, Yifan and Williams, Christopher Kazu and Magaki, Shino and Khanlou, Negar and Vinters, Harry and Chen, Zesheng and Ni, Shuo and Yang, Chunxu and Yan, Wenzhong and Zhang, Xinhai Robert and Li, Yang and Haeri, Mohammad and Chen, Xiang 'Anthony'},<br>title = {Improving Workflow Integration with XPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology},<br>year = {2022},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>issn = {1073-0516},<br>url = {https://doi.org/10.1145/3577011},<br>doi = {10.1145/3577011},<br>abstract = {Recent developments in AI have provided assisting tools to support pathologists' diagnoses. However, it remains challenging to incorporate such tools into pathologists' practice; one main concern is AI's insufficient workflow integration with medical decisions. We observed pathologists' examination and discovered that the main hindering factor to integrate AI is its incompatibility with pathologists' workflow. To bridge the gap between pathologists and AI, we developed a human-AI collaborative diagnosis tool — xPath— that shares a similar examination process to that of pathologists, which can improve AI's integration into their routine examination. The viability of xPath&nbsp;is confirmed by a technical evaluation and work sessions with twelve medical professionals in pathology. This work identifies and addresses the challenge of incorporating AI models into pathology, which can offer first-hand knowledge about how HCI researchers can work with medical professionals side-by-side to bring technological advances to medical tasks towards practical applications.},<br>note = {Just Accepted},<br>journal = {ACM Trans. Comput.-Hum. Interact.},<br>month = {dec},<br>keywords = {Human-AI collaboration; digital pathology; medical AI; meningioma}<br>}"
    thumbnail: tochi_xpath_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1ncRzqGFjiYcwFQdvmPRn_hxYcdM9VK3V/view?usp=sharing
  - name: 'CrossA11y'
    title: 'CrossA11y: Identifying Video Accessibility Issues via Cross-modal Grounding'
    authors:
      - Xingyu "Bruce" Liu (UCLA HCI Research)
      - Ruolin Wang (UCLA HCI Research)
      - Dingzeyu Li, Adobe Research
      - Xiang 'Anthony' Chen (UCLA HCI Research)
      - Amy Pavel, UT Austin
    pubs: UIST 2022 🏆
    img: t_crossa11y.jpg
    abstract: "Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). However, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos. A video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. In this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. Using cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries. CrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. We demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline."
    videoSite: vimeo
    video: 777518989
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720304162184" title="2022_CrossA11y"><img src="https://live.staticflickr.com/65535/52538161822_681d3813d7_b.jpg" width="1024" height="768" alt="2022_CrossA11y"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Xingyu \"Bruce\" Liu, Ruolin Wang, Dingzeyu Li, Xiang Anthony Chen, and Amy Pavel. 2022. CrossA11y: Identifying Video Accessibility Issues via Cross-modal Grounding. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22). Association for Computing Machinery, New York, NY, USA, Article 43, 1-14. https://doi.org/10.1145/3526113.3545703"
    bibtex: "@inproceedings{10.1145/3526113.3545703,<br>author = {Liu, Xingyu \"Bruce\" and Wang, Ruolin and Li, Dingzeyu and Chen, Xiang Anthony and Pavel, Amy},<br>title = {CrossA11y: Identifying Video Accessibility Issues via Cross-Modal Grounding},<br>year = {2022},<br>isbn = {9781450393201},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3526113.3545703},<br>doi = {10.1145/3526113.3545703},<br>abstract = {Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). However, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos. A video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. In this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. Using cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries. CrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. We demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline.},<br>booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},<br>articleno = {43},<br>numpages = {14},<br>keywords = {video, accessibility, closed caption, audio description},<br>location = {Bend, OR, USA},<br>series = {UIST '22}<br>}"
    thumbnail: uist2022_crossa11y_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1ne510PMxhpM4blDV2g16_3sKtbBRp1yF/view?usp=sharing
  - name: 'GANZilla'
    title: 'GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks'
    authors:
      - Noyan Evirgen (UCLA HCI Research)
      - Xiang 'Anthony' Chen (UCLA HCI Research)
    pubs: UIST 2022
    img: t_ganzilla.jpg
    abstract: "Generative Adversarial Network (GAN) is widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN's 'black box' nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to extract editing directions to control GAN. Complementarily, we propose a GANzilla—a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing goals. In a study with 12 participants, GANzilla users were able to discover directions that (i) edited images to match provided examples (closed-ended tasks) and that (ii) met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks)."
    videoSite: vimeo
    video: 777519565
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720304012340" title="2022_GANZilla"><img src="https://live.staticflickr.com/65535/52527432649_b2365339c5_c.jpg" width="1024" height="768" alt="2022_GANZilla"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Noyan Evirgen and Xiang 'Anthony' Chen. 2022. GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22). Association for Computing Machinery, New York, NY, USA, Article 75, 1-10. https://doi.org/10.1145/3526113.3545638"
    bibtex: "@inproceedings{10.1145/3526113.3545638,<br>author = {Evirgen, Noyan and Chen, Xiang 'Anthony'},<br>title = {GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks},<br>year = {2022},<br>isbn = {9781450393201},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3526113.3545638},<br>doi = {10.1145/3526113.3545638},<br>abstract = {Generative Adversarial Network (GAN) is widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN's 'black box' nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to extract editing directions to control GAN. Complementarily, we propose a GANzilla—a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing goals. In a study with 12 participants, GANzilla users were able to discover directions that (i)&nbsp;edited images to match provided examples (closed-ended tasks) and that (ii)&nbsp;met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks).},<br>booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},<br>articleno = {75},<br>numpages = {10},<br>keywords = {Direction Discovery, Interactive Systems, Explainable-AI, Generative Adversarial Networks},<br>location = {Bend, OR, USA},<br>series = {UIST '22}<br>}"
    paperUrl: https://drive.google.com/file/d/1nmvrkm6l1ueSIIKYXQcTC2sWSCyXEY5V/view?usp=sharing
    thumbnail: uist2022_ganzilla_thumbnail.jpg
  - name: 'EmoGlass'
    title: 'EmoGlass: an End-to-End AI-Enabled Wearable Platform for Enhancing Self-Awareness of Emotional Health'
    authors: 
      - "Zihan Yan (UCLA HCI Research, Zhejiang University)"
      - "Yufei Wu (Zhejiang University)"
      - "Yang Zhang (UCLA HCI Research)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CHI 2022
    img: t_emoglass.jpg
    abstract: "Often, emotional disorders are overlooked due to theirlack of awareness, resulting in potential mental issues. Recent advances in sensing and inference technology provide a viable path to wearable facial-expression-based emotion recognition. However, most prior work has explored only laboratory settings and few platforms are geared towards end-users in everyday lives or provide personalized emotional suggestions to promote self-regulation. We present EmoGlass, an end-to-end wearable platform that consists of emotion detection glasses and an accompanying mobile application. Our single-camera-mounted glasses can detect seven facial expressions based on partial face images. We conducted a three-day out-of-lab study (N=15) to evaluate the performance of EmoGlass. We iterated on the design of the EmoGlass application for efective self-monitoring and awareness of users' daily emotional states. We report quantitative and qualitative fndings, based on which we discuss design recommendations for future work on sensing and enhancing awareness of emotional health."
    videoSite: vimeo
    video: 715380296
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720299388332" title="2022_EmoGlass"><img src="https://live.staticflickr.com/65535/52111181659_7fafcc02cd_b.jpg" width="1024" height="768" alt="2022_EmoGlass"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Zihan Yan, Yufei Wu, Yang Zhang, Xiang 'Anthony' Chen. EmoGlass: an End-to-End AI-Enabled Wearable Platform for Enhancing Self-Awareness of Emotional Health. In CHI Conference on Human Factors in Computing Systems, pp. 1-19. 2022. https://doi.org/10.1145/3491102.3501925"
    bibtex: "@inproceedings{10.1145/3491102.3501925,<br>author = {Yan, Zihan and Wu, Yufei and Zhang, Yang and Chen, Xiang 'Anthony'},<br>title = {EmoGlass: An End-to-End AI-Enabled Wearable Platform for Enhancing Self-Awareness of Emotional Health},<br>year = {2022},<br>isbn = {9781450391573},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3491102.3501925},<br>doi = {10.1145/3491102.3501925},<br>booktitle = {CHI Conference on Human Factors in Computing Systems},<br>articleno = {13},<br>numpages = {19},<br>keywords = {Facial expression detection, Wearable, Emotion sensing, Mobile health, Mental health},<br>location = {New Orleans, LA, USA},<br>series = {CHI '22}<br>}"
    paperUrl: https://drive.google.com/file/d/1nubb0jtmIw5OzTNlNOpdrdCEcN0YWJdk/view?usp=sharing
    thumbnail: chi2022_emoglass_thumbnail.jpg
  - name: 'Roman'
    title: 'Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms'
    authors: 
      - "Jiahao Li (UCLA HCI Research)"
      - "Alexis A Samoylov (UCLA HCI Research)"
      - "Jeeeun Kim (HCIED Lab, Texas A&M University)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CHI 2022
    img: t_roman.jpg
    abstract: "One important vision of robotics is to provide physical assistance by manipulating different everyday objects, e.g., hand tools, kitchen utensils. However, many objects designed for dexterous hand-control are not easily manipulable by a single robotic arm with a generic parallel gripper. Complementary to existing research on developing grippers and control algorithms, we present Roman, a suite of hardware design and software tool support for robotic engineers to create 3D printable mechanisms attached to everyday handheld objects, making them easier to be manipulated by conventional robotic arms. The Roman hardware comes with a versatile magnetic gripper that can snap on/off handheld objects and drive add-on mechanisms to perform tasks. Roman also provides software support to register and author control programs. To validate our approach, we designed and fabricated Roman mechanisms for 14 everyday objects/tasks presented within a design space and conducted expert interviews with robotic engineers indicating that Roman serves as a practical alternative for enabling robotic manipulation of everyday objects."
    videoSite: vimeo
    video: 715062811
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72177720299386246" title="2022_Roman"><img src="https://live.staticflickr.com/65535/52111138539_48de9939d2_b.jpg" width="1024" height="768" alt="2022_Roman"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Jiahao Li, Alexis Samoylov, Jeeeun Kim, and Xiang 'Anthony' Chen. Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms. In CHI Conference on Human Factors in Computing Systems, pp. 1-17. 2022. https://doi.org/10.1145/3491102.3501818"
    bibtex: "@inproceedings{10.1145/3491102.3501818,<br>author = {Li, Jiahao and Samoylov, Alexis and Kim, Jeeeun and Chen, Xiang 'Anthony'},<br>title = {Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms},<br>year = {2022},<br>isbn = {9781450391573},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3491102.3501818},<br>doi = {10.1145/3491102.3501818},<br>booktitle = {CHI Conference on Human Factors in Computing Systems},<br>articleno = {272},<br>numpages = {17},<br>keywords = {mechanism design., handheld objects augmentation, Robotic grasping and manipulation},<br>location = {New Orleans, LA, USA},<br>series = {CHI '22}<br>}"
    paperUrl: https://drive.google.com/file/d/1o2XaOEhrGwrF4DPfX-6LEXfvo4789IcJ/view?usp=sharing
    thumbnail: chi2022_roman_thumbnail.jpg
  - name: 'Impetus'
    title: 'Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists'
    authors: 
      - "Hongyan Gu (UCLA HCI Research)"
      - "Jingbin Huang (UCLA HCI Research)"
      - "Lauren Hung (CMU HCII)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CSCW 2021
    img: t_impetus.jpeg
    abstract: "Despite the promises of data-driven artificial intelligence (AI), little is known about how we can bridge the gulf between traditional physician-driven diagnosis and a plausible future of medicine automated by AI. Specifically, how can we involve AI usefully in physicians' diagnosis workflow given that most AI is still nascent and error-prone (e.g., in digital pathology)? To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which we prototype Impetus — a tool where an AI takes various degrees of initiatives to provide various forms of assistance to a pathologist in detecting tumors from histological slides. We summarize observations and lessons learned from a study with eight pathologists and discuss recommendations for future work on human-centered medical AI systems."
    videoSite: vimeo
    video: 361729363
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157718192437332" title="2021_Impetus"><img src="https://live.staticflickr.com/65535/50916469012_0ecf9689ec_b.jpg" width="1024" height="768" alt="2021_Impetus"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Hongyan Gu, Jingbin Huang, Lauren Hung, and Xiang 'Anthony' Chen. 2021. Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists. Proc. ACM Hum.-Comput. Interact. 5, CSCW 1, Article 10 (April 2021), 25 pages. https://doi.org/10.1145/3449084"
    bibtex: "@article{10.1145/3449084,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Gu, Hongyan and Huang, Jingbin and Hung, Lauren and Chen, Xiang Anthony},<br>&nbsp; &nbsp; &nbsp; &nbsp;journal={Proceedings of the ACM on Human-computer Interaction},<br>&nbsp; &nbsp; &nbsp; &nbsp;volume={5},<br>&nbsp; &nbsp; &nbsp; &nbsp;number={CSCW},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={1--25},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2021},<br>&nbsp; &nbsp; &nbsp; &nbsp;issue_date = {April 2021},<br>&nbsp; &nbsp; &nbsp; &nbsp;publisher = {Association for Computing Machinery},<br>&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3449084},<br>&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3449084},<br>&nbsp; &nbsp; &nbsp; &nbsp;articleno = {10},<br>&nbsp; &nbsp; &nbsp; &nbsp;numpages = {25}<br>}"
    paperUrl: 'https://drive.google.com/file/d/1-Vni8pVYMZZdG_vX0flu3vrXdL1ufrds/view?usp=sharing'
    thumbnail: cscw2021_impetus_thumbnail.jpg
  - name: 'Va11y'
    title: 'What Makes Videos Accessible to Blind and Visually Impaired People?'
    authors:
      - "Xingyu Liu (UCLA HCI Research)"
      - "Patrick Carrington (Carnegie Mellon University)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Amy Pavel (Carnegie Mellon University)"
    pubs: CHI 2021
    img: t_va11y.jpeg
    abstract: "User-generated videos are an increasingly important source of information online, yet most online videos are inaccessible to blind and visually impaired (BVI) people. To find videos that are accessible, or understandable without additional description of the visual content, BVI people in our formative studies reported that they used a time-consuming trial-and-error approach: clicking on a video, watching a portion, leaving the video, and repeating the process. BVI people also reported video accessibility heuristics that characterize accessible and inaccessible videos. We instantiate 7of the identified heuristics (2 audio-related, 2 video-related, and 3audio-visual) as automated metrics to assess video accessibility. We collected a dataset of accessibility ratings of videos by BVI people and found that our automatic video accessibility metrics correlated with the accessibility ratings (Adjusted R^2= 0.642). We augmented a video search interface with our video accessibility metrics and predictions. BVI people using our augmented video search interface selected an accessible video more efficiently than when using the original search interface. By integrating video accessibility metrics, video hosting platforms could help people surface accessible videos and encourage content creators to author more accessible products, improving video accessibility for all."
    videoSite: vimeo
    video: 551259813
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157719196580808" title="2021_Va11y"><img src="https://live.staticflickr.com/65535/51184008988_1bdae2250c_b.jpg" width="1024" height="768" alt="2021_Va11y"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Xingyu Liu, Patrick Carrington, Xiang 'Anthony' Chen, and Amy Pavel. 2021. What Makes Videos Accessible to Blind and Visually Impaired People? In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 272, 1-14. DOI:https://doi.org/10.1145/3411764.3445233"
    bibtex: "@inproceedings{10.1145/3411764.3445233,<br>author = {Liu, Xingyu and Carrington, Patrick and Chen, Xiang 'Anthony' and Pavel, Amy},<br>title = {What Makes Videos Accessible to Blind and Visually Impaired People?},<br>year = {2021},<br>isbn = {9781450380966},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3411764.3445233},<br>doi = {10.1145/3411764.3445233},<br>abstract = { User-generated videos are an increasingly important source of information online, yet most online videos are inaccessible to blind and visually impaired (BVI) people. To find videos that are accessible, or understandable without additional description of the visual content, BVI people in our formative studies reported that they used a time-consuming trial-and-error approach: clicking on a video, watching a portion, leaving the video, and repeating the process. BVI people also reported video accessibility heuristics that characterize accessible and inaccessible videos. We instantiate 7 of the identified heuristics (2 audio-related, 2 video-related, and 3 audio-visual) as automated metrics to assess video accessibility. We collected a dataset of accessibility ratings of videos by BVI people and found that our automatic video accessibility metrics correlated with the accessibility ratings (Adjusted R2 = 0.642). We augmented a video search interface with our video accessibility metrics and predictions. BVI people using our augmented video search interface selected an accessible video more efficiently than when using the original search interface. By integrating video accessibility metrics, video hosting platforms could help people surface accessible videos and encourage content creators to author more accessible products, improving video accessibility for all.},<br>booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},<br>articleno = {272},<br>numpages = {14},<br>keywords = {accessibility, visual impairments, online videos, blind},<br>location = {Yokohama, Japan},<br>series = {CHI '21}<br>}"
    paperUrl: https://drive.google.com/file/d/1o4Blw_T10WPVB56uTbyGAXF7gCXAYIUp/view?usp=sharing
    thumbnail: chi2021_va11y_thumbnail.jpg
  - name: 'Revamp'
    title: 'Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users'
    authors:
      - "Ruolin Wang (UCLA HCI Research)"
      - "Zixuan Chen (UCLA HCI Research)"
      - "Mingrui 'Ray' Zhang (The Information School, University of Washington)"
      - "Zhaoheng Li (Department of Computer Science and Technology, Tsinghua University)"
      - "Zhixiu Liu (Computer Science Department, Stanford University)"
      - "Zihan Dang (UCLA HCI Research)"
      - "Chun Yu (Department of Computer science and Technology, Tsinghua University)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CHI 2021
    img: t_revamp.jpeg
    abstract: "Online shopping has become a valuable modern convenience, but blind or low vision (BLV) users still face significant challenges using it, because of: 1) inadequate image descriptions and 2) the inability to filter large amounts of information using screen readers. To address those challenges, we propose Revamp, a system that leverages customer reviews for interactive information retrieval. Revamp is a browser integration that supports review-based question-answering interactions on a reconstructed product page. From our interview, we identified four main aspects (color, logo, shape, and size) that are vital for BLV users to understand the visual appearance of a product. Based on the findings, we formulated syntactic rules to extract review snippets, which were used to generate image descriptions and responses to users' queries. Evaluations with eight BLV users showed that Revamp 1) provided useful descriptive information for understanding product appearance and 2) helped the participants locate key information efficiently."
    videoSite: vimeo
    video: 515072781
    album: '72157718382497033'
    citation: "Ruolin Wang, Zixuan Chen, Mingrui Zhang, Mingrui Ray Zhang, Zhaoheng Li, Zhixiu Liu, Zihan Dang, Chun Yu, and Xiang 'Anthony' Chen. 2021. Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1-14. CHI '21. New York, NY, USA: ACM, 2021. https://doi.org/10.1145/3411764.3445547."
    bibtex: "@inproceedings{Wang:2021:Revamp,<br>&nbsp; &nbsp; &nbsp; &nbsp;author = {Wang, Ruolin and Chen, Zixuan and Zhang, Mingrui and Zhang, Mingrui Ray and Li, Zhaoheng and Liu, Zhixiu and Dang, Zihan and Yu, Chun and Chen, Xiang Anthony.},<br>&nbsp; &nbsp; &nbsp; &nbsp;title = {Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users},<br>&nbsp; &nbsp; &nbsp; &nbsp;year = {2021},<br>&nbsp; &nbsp; &nbsp; &nbsp;isbn = {978-1-4503-8096-6/21/05},<br>&nbsp; &nbsp; &nbsp; &nbsp;publisher = {ACM},<br>&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3411764.3445547},<br>&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3411764.3445547},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages = {1-14},<br>&nbsp; &nbsp; &nbsp; &nbsp;numpages = {14},<br>&nbsp; &nbsp; &nbsp; &nbsp;keywords = {Online shopping; Information Retrieval; Accessibility; Blind or Low Vision Users; Reviews; Image Description; Question-answering},<br>&nbsp; &nbsp; &nbsp; &nbsp;location = {Tokohama, Japan},<br>&nbsp; &nbsp; &nbsp; &nbsp;series = {CHI '21}<br>}"
    paperUrl: https://drive.google.com/file/d/1oGtoOrICzzVCBQb0otNfTbUZRlxg7Mh4/view?usp=sharing
    thumbnail: chi2021_revamp_thumbnail.jpg
  - name: 'XAlgo'
    title: "XAlgo: a Design Probe of Explaining Algorithms' Internal States via Question-Answering"
    authors:
      - "Juan Rebanal (UCLA HCI Research)"
      - "Jordan Combitsis (UCLA HCI Research)"
      - "Yuqi Tang (UCLA HCI Research)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: IUI 2021
    img: t_xalgo.jpg
    abstract: "Algorithms often appear as 'black boxes' to non-expert users. While prior work focuses on explainable representations and expert-oriented exploration, we propose and study an interactive approach using question answering to explain deterministic algorithms to non-expert users who need to understand the algorithms' internal states (e.g., students learning algorithms, operators monitoring robots, admins troubleshooting network routing). We construct XAlgo---a formal model that first classifies the type of question based on a taxonomy and generates an answer based on a set of rules that extract information from representations of an algorithm's internal states, e.g., the pseudocode. A design probe in an algorithm learning scenario with 18 participants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports findings and design implications based on what kinds of questions people ask, how well XAlgo responds, and what remain as challenges to bridge users' gulf of understanding algorithms."
    album: '72157718472175126'
    citation: "Juan Rebanal, Jordan Combitsis, Yuqi Tang, Xiang 'Anthony' Chen. XAlgo: a Design Probe of Explaining Algorithms' Internal States via Question-Answering. In Proceedings of the 26th International Conference on Intelligent User Interfaces (IUI '21), April 14--17, 2021, College Station, TX, USA. https://doi.org/10.1145/3397481.3450676."
    bibtex: "@inproceedings{10.1145/2856767.2856812,<br>&nbsp; &nbsp; &nbsp; &nbsp;author = {Rebanal, Juan and Tang, Yuqi and Combitsis, Jordan and Chen, Xiang 'Anthony'},<br>&nbsp; &nbsp; &nbsp; &nbsp;title = {XAlgo: a Design Probe of Explaining Algorithms' Internal States via Question-Answering},<br>&nbsp; &nbsp; &nbsp; &nbsp;year = {2021},<br>&nbsp; &nbsp; &nbsp; &nbsp;isbn = {978-1-4503-8017-1/21/04},<br>&nbsp; &nbsp; &nbsp; &nbsp;publisher = {Association for Computing Machinery},<br>&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3397481.3450676},<br>&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3397481.3450676},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle = {Proceedings of the 26st International Conference on Intelligent User Interfaces},<br>&nbsp; &nbsp; &nbsp; &nbsp;series = {IUI '21}<br>}"
    paperUrl: 'https://drive.google.com/file/d/1RFlYYoi9ycO2g4WalkjM3NBtF_PjDb0x/view?usp=sharing'
    thumbnail: iui2021_xalgo_thumbnail.jpg
  - name: 'OralViewer'
    title: "OralViewer: 3D Demonstration of Dental Surgeries for Patient Education with Oral Cavity Reconstruction from a 2D Panoramic X-ray"
    authors:
      - "Yuan Liang (UCLA HCI Research)"
      - "Liang Qiu (UCLA)"
      - "Tiancheng Lu (University of Pittsburgh)"
      - "Zhujun Fang, Dezhan Tu, Jiawei Yang, Yiting Shao, Kun Wang (UCLA)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Lei He (UCLA)"
    pubs: IUI 2021
    img: t_oralviewer.jpg
    abstract: "Patient's understanding on forthcoming dental surgeries is required by patient-centered care and helps reduce anxiety. Due to the complexity of dental surgeries and the patient-dentist expertise gap, conventional techniques of patient education are usually not effective for explaining surgical steps. In this paper, we present OralViewer—the first interactive application that enables dentist's demonstration of dental surgeries in 3D to promote patients' understanding. OralViewer takes a single 2D panoramic dental X-ray to reconstruct patient-specific 3D teeth structures, which are then assembled with registered gum and jaw bone models for complete oral cavity modeling. During the demonstration, OralViewer enables dentists to show surgery steps with virtual dental instruments that can animate effects on a 3D model in real-time. A technical evaluation shows that our deep learning model achieves a mean Intersection over Union (IoU) of 0.771 for 3D teeth reconstruction. A patient study with 12 participants shows OralViewer can improve patients' understanding of surgeries. A preliminary expert study with 3 board-certified dentists further verifies the clinical validity of our system."
    videoSite: vimeo
    video: 553825921
    albumEmbedCode: '72157<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157719248545536" title="2021_OrlaViewer"><img src="https://live.staticflickr.com/65535/51197471885_d232eaca78_b.jpg" width="1024" height="768" alt="2021_OrlaViewer"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>719248545536'
    citation: "Yuan Liang, Liang Qiu, Tiancheng Lu, Zhujun Fang, Dezhan Tu, Jiawei Yang, Yiting Shao, Kun Wang, Xiang 'Anthony' Chen, and Lei He. 2021. OralViewer: 3D Demonstration of Dental Surgeries for Patient Education with Oral Cavity Reconstruction from a 2D Panoramic X-ray. In 26th International Conference on Intelligent User Interfaces (IUI '21). Association for Computing Machinery, New York, NY, USA, 553-563. DOI:https://doi.org/10.1145/3397481.3450695"
    bibtex: "@inproceedings{10.1145/3397481.3450695,<br>author = {Liang, Yuan and Qiu, Liang and Lu, Tiancheng and Fang, Zhujun and Tu, Dezhan and Yang, Jiawei and Shao, Yiting and Wang, Kun and Chen, Xiang 'Anthony' and He, Lei},<br>title = {OralViewer: 3D Demonstration of Dental Surgeries for Patient Education with Oral Cavity Reconstruction from a 2D Panoramic X-Ray},<br>year = {2021},<br>isbn = {9781450380171},<br>publisher = {Association for Computing Machinery},<br>address = {New York, NY, USA},<br>url = {https://doi.org/10.1145/3397481.3450695},<br>doi = {10.1145/3397481.3450695},<br>abstract = { Patient's understanding on forthcoming dental surgeries is required by patient-centered care and helps reduce anxiety. Due to the complexity of dental surgeries and the patient-dentist expertise gap, conventional techniques of patient education are usually not effective for explaining surgical steps. In this paper, we present OralViewer—the first interactive application that enables dentist's demonstration of dental surgeries in 3D to promote patients' understanding. OralViewer takes a single 2D panoramic dental X-ray to reconstruct patient-specific 3D teeth structures, which are then assembled with registered gum and jaw bone models for complete oral cavity modeling. During the demonstration, OralViewer enables dentists to show surgery steps with virtual dental instruments that can animate effects on a 3D model in real-time. A technical evaluation shows that our deep learning model achieves a mean Intersection over Union (IoU) of 0.771 for 3D teeth reconstruction. A patient study with 12 participants shows OralViewer can improve patients' understanding of surgeries. A preliminary expert study with 3 board-certified dentists further verifies the clinical validity of our system.},<br>booktitle = {26th International Conference on Intelligent User Interfaces},<br>pages = {553-563},<br>numpages = {11},<br>keywords = {deep learning, 3D visualization, patient education},<br>location = {College Station, TX, USA},<br>series = {IUI '21}<br>}<br><br>"
    paperUrl: 'https://drive.google.com/file/d/1-yO3swmeSoz0uxDNkdStD2iIbbHhSGSP/view?usp=sharing'
    thumbnail: iui2021_oralviewer.thumbnail.jpg
  - name: 'DualVib'
    title: 'DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback'
    authors:
      - "Yudai Tanaka (University of Chicago)"
      - "Arata Horie (The University of Tokyo)"
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: VRST 2020
    img: t_dualvib.jpg
    abstract: "We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user's hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user's hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object's surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device."
    videoSite: vimeo
    video: 455326878
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157715869674293" title="2020_DualVib"><img src="https://live.staticflickr.com/65535/50314078682_b3d2e30eb7_b.jpg" width="1024" height="768" alt="2020_DualVib"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Yudai Tanaka, Arata Horie, Xiang 'Anthony' Chen. 2020. DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback. In 26th ACM Symposium on Virtual Reality Software and Technology, 1-10. VRST '20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3385956.3418964."
    bibtex: "@inproceedings{10.1145/3385956.3418964,<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;author = {Tanaka, Yudai and Horie, Arata and Chen, Xiang 'Anthony'},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;title = {DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;year = {2020},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;isbn = {9781450376198},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;publisher = {Association for Computing Machinery},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3385956.3418964},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3385956.3418964},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;booktitle = {26th ACM Symposium on Virtual Reality Software and Technology},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;pages = {1-10},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;numpages = {10},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;keywords = {haptics, virtual reality, mass perception, vibration},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;location = {Virtual Event, Canada},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;series = {VRST '20}<br>}"
    paperUrl: 'https://drive.google.com/file/d/1aEnQ-LSWe7upFDG1-UQ95tJ8juRcbvnf/view?usp=sharing'
    thumbnail: vrst2020_dualvib_thumbnail.jpg
  - name: 'Geno'
    title: 'Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications'
    pubs: UIST 2020
    img: t_geno.jpg
    authors:
      - 'Ritam Jyoti Sarmah (UCLA HCI Research)'
      - 'Yunpeng Ding (UCLA HCI Research)'
      - 'Di Wang (UCSD Computer Science)'
      - 'Cheuk Yin Phipson Lee (UCLA HCI Research)'
      - 'Toby Jia-Jun Li (CMU HCII)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    abstract: "Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUI-based web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multimodal interfaces. We present Geno---a developer tool for adding the voice input modality to existing web apps without requiring significant NLP expertise. Geno provides a high-level workflow for developers to specify functionalities to be supported by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports multimodal references to GUI context in voice commands (e.g. 'move this [event] to next week' while pointing at an event with the cursor). In a study, developers with little NLP expertise were able to add multimodal voice command support for two existing web apps using Geno."
    videoSite: vimeo
    video: 440213846
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157716157681577" title="2020_Geno"><img src="https://live.staticflickr.com/65535/50391995692_ef836860fe_b.jpg" width="1024" height="768" alt="2020_Geno"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    paperUrl: https://drive.google.com/file/d/1oZCvo5Q1sgRNK9HLU6ea91ghGqOy8_xD/view?usp=sharing
    citation: "Sarmah, R.J., Ding, Y., Wang, D., Lee, C.Y.P., Li, T.J.J. and Chen, X.A., 2020, October. Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (pp. 1169-1181)."
    bibtex: "@inproceedings{sarmah2020geno,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Sarmah, Ritam Jyoti and Ding, Yunpeng and Wang, Di and Lee, Cheuk Yin Phipson and Li, Toby Jia-Jun and Chen, XiangAnthony},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={1169--1181},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2020}<br>}"
    thumbnail: uist2020_geno_thumbnail.jpg
  - name: 'Romeo'
    title: 'Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities'
    authors:
      - 'Jiahao Li (UCLA HCI Research)'
      - 'Meilin Cui (UCLA HCI Research)'
      - 'Jeeeun Kim (Texas A&M University)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: UIST 2020
    img: t_romeo.jpg
    abstract: "Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice."
    videoSite: vimeo
    video: 459497267
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157716023426003" title="2020_Romeo"><img src="https://live.staticflickr.com/65535/50356908336_99acc02a6f_b.jpg" width="1024" height="768" alt="2020_Romeo"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    paperUrl: 'https://drive.google.com/file/d/1cOdjdHvsdquguuCuPzf0sriFZ2buVpLD/view?usp=sharing'
    citation: "Li, J., Cui, M., Kim, J. and Chen, X.A., 2020, October. Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (pp. 897-911)."
    bibtex: "@inproceedings{li2020romeo,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Li, Jiahao and Cui, Meilin and Kim, Jeeeun and Chen, XiangAnthony},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={897--911},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2020}<br>}<br>"
    thumbnail: uist2020_romeo_thumbnail.jpg
  - name: 'CheXplain'
    title: 'CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis'
    authors:
      - 'Yao Xie (UCLA HCI Research)'
      - 'Melody Chen (UCLA HCI Research)'
      - 'David Kao (UCLA HCI Research)'
      - 'Ge Gao (University of Maryland, College Park)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CHI 2020
    img: t_chexplain.jpg
    abstract: "The recent development of data-driven AI promises to automate medical diagnosis; however, most AI functions as 'black boxes' to physicians with limited computational knowledge. Using medical imaging as a point of departure, we conducted three research activities to formulate the design of CheXplain---a system that enables physicians to explore and understand AI-enabled chest X-ray analysis: (i) a paired survey between referring physicians and radiologists reveals whether, when, and what kinds of explanations are needed; (ii) a low-fidelity prototype co-designed with three physicians formulates eight key features; and (iii) a high-fidelity prototype evaluated by another six medical professionals provides detailed summative insights on how each feature enables the exploration and understanding of AI. We summarize by discussing recommendations for future work to design and implement explainable medical AI systems that encompass four recurring themes: motivation, constraint, explanation, and justification."
    videoSite: vimeo
    video: 416564621
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157714227177278" title="2020_CheXplain"><img src="https://live.staticflickr.com/65535/49872248883_7e64f82c14_b.jpg" width="1024" height="768" alt="2020_CheXplain"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Yao Xie, Melody Chen, David Kao, Ge Gao, and Xiang 'Anthony' Chen. 2020. CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1-13. DOI:https://doi.org/10.1145/3313831.3376807"
    paperUrl: https://drive.google.com/file/d/1op5naceGh4rZRYL5UHC2eFWCrtD_IZD9/view?usp=sharing
    bibtex: "@inproceedings{10.1145/3313831.3376807,<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;author = {Xie, Yao and Chen, Melody and Kao, David and Gao, Ge and Chen, Xiang 'Anthony'},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;title = {CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;year = {2020},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;isbn = {9781450367080},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;publisher = {Association for Computing Machinery},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3313831.3376807},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3313831.3376807},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;pages = {1-13},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;numpages = {13},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;keywords = {explainable artificial intelligence, physician-centered design, system design},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;location = {Honolulu, HI, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;series = {CHI '20}<br>}"
    thumbnail: chi2020_chexplain_thumbnail.jpg
  - name: 'OralCam'
    title: 'OralCam: Enabling Self-Examination and Awareness of Oral Health Using a Smartphone Camera'
    authors:
      - 'Yuan Liang (UCLA HCI Research & Design Automation Lab)'
      - 'Hsuan Wei Fan (Tsinghua University)'
      - 'Zhujun Fang (UC Davis)'
      - 'Leiying Miao,  Wen Li,  Xuan Zhang,  Weibin Sun (Nanjing Stomatological Hospital, Meidcal School)'
      - 'Kun Wang, Lei He (UCLA Design Automation Lab)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: CHI 2020 🏅
    img: t_oralcam.jpg
    abstract: Due to a lack of resources or awareness, oral diseases are often left unexamined and untreated, affecting a large population worldwide. With the advent of low-cost, sensor-equipped smartphones, mobile apps offer a promising possibility. However, to the best of our knowledge, no mobile health (mHealth) solutions can directly support users to self-examine their oral health condition. We present OralCam, the first interactive app that enables end-users' self-examination of five common oral conditions (diseases or early disease signals) by taking smartphone photos of one's oral cavity. OralCam allows a user to annotate additional information to augment the input image, and presents the output hierarchically, probabilistically and with visual explanations to help laymen users understand examination results. We describe a deep learning backend trained on a data set that consists of 3,182 oral photos. We report a technical evaluation, a week-long in-the-wild user study and an expert interview to validate OralCam.
    citation: "Yuan Liang, Hsuan Wei Fan, Zhujun Fang, Leiying Miao, Wen Li, Xuan Zhang, Weibin Sun, Kun Wang, Lei He, and Xiang 'Anthony' Chen. 2020. OralCam: Enabling Self-Examination and Awareness of Oral Health Using a Smartphone Camera. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1-13. DOI:https://doi.org/10.1145/3313831.3376238"
    bibtex: "@inproceedings{10.1145/3313831.3376238,<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;author = {Liang, Yuan and Fan, Hsuan Wei and Fang, Zhujun and Miao, Leiying and Li, Wen and Zhang, Xuan and Sun, Weibin and Wang, Kun and He, Lei and Chen, Xiang 'Anthony'},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;title = {OralCam: Enabling Self-Examination and Awareness of Oral Health Using a Smartphone Camera},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;year = {2020},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;isbn = {9781450367080},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;publisher = {Association for Computing Machinery},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;address = {New York, NY, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;url = {https://doi.org/10.1145/3313831.3376238},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;doi = {10.1145/3313831.3376238},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;pages = {1-13},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;numpages = {13},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;keywords = {mobile health, oral health, artificial intelligence, deep learning},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;location = {Honolulu, HI, USA},<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;series = {CHI '20}<br>}"
    videoSite: vimeo
    video: 416571045
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157714227638632" title="2020_OralCam"><img src="https://live.staticflickr.com/65535/49872175903_901e4b13a4_b.jpg" width="1024" height="768" alt="2020_OralCam"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    paperUrl: https://drive.google.com/file/d/1osmD5ly8LsYcFGOIlnNAGKnJ3-4dsQI5/view?usp=sharing
    thumbnail: chi2020_oralcam_thumbnail.jpg
  - name: 'Robiot'
    title: 'Robiot: A Design Tool for Actuating Everyday Object with Automatically Generated 3D Printable Mechanism'
    authors:
      - 'Jiahao Li (UCLA HCI Research)'
      - 'Jeeeun Kim (Texas A&M University)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    pubs: UIST 2019
    img: t_robiot.jpg
    abstract: Users can now easily communicate information with an Internet of Things; in contrast, there remains a lack of support to automate tasks that involve legacy static objects, e.g. adjusting a desk lamp's angle for optimal brightness, turning on/off a manual faucet when washing dishes, sliding a window to maintain a preferred indoor temperature. Automating these simple physical tasks has the potential to improve people's quality of life, which is particularly important for people with a disability or in situational impairment.<br/><br/>We present Robiot -- a design tool for generating mechanisms that can be attached to, motorized, and actuating legacy static objects to perform simple physical tasks. Users only need to take a short video manipulating an object to demonstrate an intended physical behavior. Robiot then extracts requisite parameters and automatically generates 3D models of the enabling actuation mechanisms by performing a scene and motion analysis of the 2D video in alignment with the object's 3D model. In an hour-long design session, six participants used Robiot to actuate seven everyday objects, imbuing them with the robotic capability to automate various physical tasks.
    videoSite: vimeo
    video: 358553023
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157710743420892" title="2019_Robiot"><img src="https://live.staticflickr.com/65535/48695256853_47d2c4933b_b.jpg" width="1024" height="768" alt="2019_Robiot"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Li, J., Kim, J., & Chen, X. “Anthony.” (2019). Robiot&#58; A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms. In Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology (pp. 673-685). New York, NY, USA: ACM. https://doi.org/10.1145/3332165.3347894"
    bibtex: "@inproceedings{Li:2019:RDT:3332165.3347894,<br> author = {Li, Jiahao and Kim, Jeeeun and Chen, Xiang 'Anthony'},<br> title = {Robiot&amp;\#58; A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms},<br> booktitle = {Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology},<br> series = {UIST '19},<br> year = {2019},<br> isbn = {978-1-4503-6816-2},<br> location = {New Orleans, LA, USA},<br> pages = {673--685},<br> numpages = {13},<br> url = {http://doi.acm.org/10.1145/3332165.3347894},<br> doi = {10.1145/3332165.3347894},<br> acmid = {3347894},<br> publisher = {ACM},<br> address = {New York, NY, USA},<br> keywords = {actuation, design tool, everyday objects, generative design},<br>} <br>"
    paperUrl: https://drive.google.com/file/d/1oz_P7ZC7phdFP3bxJATJnwWGlnNi6fZP/view?usp=sharing
    thumbnail: uist2019_robiot_thumbnail.jpg
  - name: Minuet
    pubs: SUI 2019
    authors:
      - 'Runchang Kang & Anhong Guo (Carnegie Mellon University)'
      - 'Gierard Laput (Apple)'
      - 'Yang Li (Google)'
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
    img: t_minuet.jpg
    title: 'Minuet: Multimodal Interaction with an Internet of Things'
    abstract: A large number of Internet-of-Things (IoT) devices will soon populate our physical environments. Yet, IoT devices' reliance on mobile applications and voice-only assistants as the primary interface limits their scalability and expressiveness. Building off of the classic 'Put-That-There' system, we contribute an exploration of the design space of voice + gesture interaction with spatially-distributed IoT devices. Our design space decomposes users' IoT commands into two components---selection and interaction. We articulate how the permutations of voice and freehand gesture for these two components can complementarily afford interaction possibilities that go beyond current approaches. We instantiate this design space as a proof-of-concept sensing platform and demonstrate a series of novel IoT interaction scenarios, such as making 'dumb' objects smart, commanding robotic appliances, and resolving ambiguous pointing at cluttered devices.
    videoSite: vimeo
    video: 358550776
    albumEmbedCode: '<a data-flickr-embed="true" href="https://www.flickr.com/photos/169665896@N06/albums/72157710744632177" title="2019_Minuet"><img src="https://live.staticflickr.com/65535/48695800733_5a27d927f8_b.jpg" width="1024" height="768" alt="2019_Minuet"/></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>'
    citation: "Kang, R., Guo, A., Laput, G., Li, Y., & Chen, X. “Anthony.” (2019). Minuet: Multimodal Interaction with an Internet of Things. In Symposium on Spatial User Interaction (pp. 2:1--2:10). New York, NY, USA: ACM. https://doi.org/10.1145/3357251.3357581"
    bibtex: "@inproceedings{Kang:2019:MMI:3357251.3357581,<br>address = {New York, NY, USA},<br>author = {Kang, Runchang and Guo, Anhong and Laput, Gierad and Li, Yang and Chen, Xiang 'Anthony'},<br>booktitle = {Symposium on Spatial User Interaction},<br>doi = {10.1145/3357251.3357581},<br>isbn = {978-1-4503-6975-6},<br>keywords = { gesture, multimodal interaction, voice,Internet-of-Things},<br>pages = {2:1----2:10},<br>publisher = {ACM},<br>series = {SUI '19},<br>title = {{Minuet: Multimodal Interaction with an Internet of Things}},<br>url = {http://doi.acm.org/10.1145/3357251.3357581},<br>year = {2019}<br>}<br>"
    paperUrl: https://drive.google.com/file/d/1p7JENXWG1UF0z2NSme6ukKncUCC6zTY7/view?usp=sharing
    thumbnail: sui2019_minuet_thumbnail.jpg
  - name: Forte
    pubs: CHI 2018
    img: t_forte.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Ye Tao (Zhejiang University)"
      - "Guanyun Wang (Carnegie Mellon University)"
      - "Runchang Kang (Carnegie Mellon University)"
      - "Tovi Grossman (University of Toronto)"
      - "Stelian Coros (ETH Zurich)"
      - "Scott Hudson (Carnegie Mellon University)"
    title: 'Forte: User-Driven Generative Design'
    abstract: Low-cost fabrication machines (e.g., 3D printers) offer the promise of creating custom-designed objects by a range of users. To maximize performance, generative design methods such as topology optimization can automatically optimize properties of a design based on high-level specifications. Though promising, such methods require people to map their design ideas--often unintuitively--to a small number of mathematical input parameters, and the relationship between those parameters and a generated design is often unclear, making it difficult to iterate a design. We present Forte, a sketch-based, real-time interactive tool for people to directly express and iterate on their designs via 2D topology optimization. Users can ask the system to add structures, provide a variation with better performance, or optimize internal material layouts. Users can globally control how much to `deviate' from the initial sketch, or perform local suggestive editing, which interactively prompts the system to update based on the new information. Design sessions with 10 participants demonstrate that Forte empowers designers to create and explore a range of optimized designs with custom forms and styles.
    video: '244079629'
    videoSite: vimeo
    album: '72157668597613889'
    citation: "Chen, X., Tao, Y., Wang, G., Kang, R., Grossman, T., Coros, S., & Hudson, S. E. (2018). Forte: User-Driven Generative Design. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (p. 496)."
    bibtex: "@inproceedings{chen2018forte,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Forte: User-Driven Generative Design},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Tao, Ye and Wang, Guanyun and Kang, Runchang and Grossman, Tovi and Coros, Stelian and Hudson, Scott E},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={496},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2018},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}"
    paperUrl: https://drive.google.com/file/d/1p9HCxYxvSfItqZXtzGbq8TOJ-ZDMFQaA/view?usp=sharing
    thumbnail: chi2018_forte_thumbnail.jpg
  - name: Medley
    pubs: CHI 2018
    img: t_medley.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Stelian Coros (ETH Zurich)"
      - "Scott Hudson (Carnegie Mellon University)"
    title: 'Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects'
    abstract: In our everyday life, we interact with and benefit from objects with a wide range of material properties. In contrast, personal fabrication machines (e.g., desktop 3D printers) currently only support a much smaller set of materials. Our goal is to close the gap between current limitations and the future of multi-material printing by enabling people to explore the reuse of material from everyday objects into their custom designs. To achieve this, we develop a library of embeddables--everyday objects that can be cut, worked and embedded into 3D printable designs. We describe a design space that characterizes the geometric and material properties of embeddables. We then develop Medley---a design tool whereby users can import a 3D model, search for embeddables with desired material properties, and interactively edit and integrate their geometry to fit into the original design. Medley also supports the final fabrication and embedding process, including instructions for carving or cutting the objects, and generating optimal paths for inserting embeddables. To validate the expressiveness of our library, we showcase numerous examples augmented by embeddables that go beyond the objects' original printed materials.
    video: '252273058'
    videoSite: vimeo
    album: '72157689656591312'
    citation: "Chen, X., Coros, S., & Hudson, S. E. (2018). Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (p. 162)."
    bibtex: "@inproceedings{chen2018medley,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Coros, Stelian and Hudson, Scott E},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={162},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2018},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}"
    paperUrl: https://drive.google.com/file/d/1pGNyS9_daDtggz8wG0PcU33GCfaHI4-R/view?usp=sharing
    thumbnail: chi2018_medley_thumbnail.jpg
  - img: t_improv.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Yang Li (Google)"
    name: Improv
    descp: An input framework for users to create cross-device gestures.
    title: >-
      Improv: An Input Framework for Improvising Cross-Device Interaction by
      Demonstration
    abstract: >-
      As computing devices become increasingly ubiquitous, it is now possible to combine the unique capabilities of different devices or Internet of Things to accomplish a task. However, there is currently a high technical barrier for creating cross-device interaction. This is especially challenging for end users who have limited technical expertise—end users would greatly benefit from custom cross-device interaction that best suits their needs. In this article, we present Improv, a cross-device input framework that allows a user to easily leverage the capability of additional devices to create new input methods for an existing, unmodified application, e.g., creating custom gestures on a smartphone to control a desktop presentation application. Instead of requiring developers to anticipate and program these cross-device behaviors in advance, Improv enables end users to improvise them on the fly by simple demonstration, for their particular needs and devices at hand. We showcase a range of scenarios where Improv is used to create a diverse set of useful cross-device input. Our study with 14 participants indicated that on average it took a participant 10 seconds to create a cross-device input technique. In addition, Improv achieved 93.7% accuracy in interpreting user demonstration of a target UI behavior by looking at the raw input events from a single example.
    bibtex: >-
      @article{chen2017improv,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Improv: An
      Input Framework for Improvising Cross-Device Interaction by
      Demonstration},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang'Anthony'
      and Li, Yang},<br>&nbsp; &nbsp; &nbsp; &nbsp;journal={ACM Transactions on
      Computer-Human Interaction (TOCHI)},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;volume={24},<br>&nbsp; &nbsp; &nbsp; &nbsp;number={2},<br>&nbsp;
      &nbsp; &nbsp; &nbsp;pages={15},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2017},<br>&nbsp; &nbsp; &nbsp; &nbsp;publisher={ACM}<br>}
    video: '216769849'
    videoSite: vimeo
    album: '72157664092225828'
    thumbnail: tochi17_improv_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1pUC9fDuo16anNqycLG63THZLFfJDq0eS/view?usp=sharing
    citation: >-
      Chen, X., & Li, Y. (2017). Improv: An Input Framework for Improvising Cross-Device Interaction by Demonstration. ACM Transactions on Computer-Human Interaction (TOCHI), 24(2), 15.
    pubs: TOCHI '17
  - img: t_reprise.jpg
    name: Reprise
    pubs: UIST 2016
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Jeeeun Kim (Texas A&M University)"
      - "Tovi Grossman (University of Toronto)"
      - "Stelian Coros (ETH Zurich)"
      - "Scott Hudson (Carnegie Mellon University)"
    title: >-
      Reprise: A Design Tool for Specifying, Generating, and Customizing 3D
      Printable Adaptations on Everyday Objects
    pubs: UIST '16
    video: '182221122'
    videoSite: vimeo
    abstract: >-
      In this paper, we describe Reprise--a design tool for specifying,
      generating, customizing and fitting adaptations onto existing household
      objects. Reprise allows users to express at a high level what type of
      action is applied to an object.  Based on this high level specification,
      Reprise automatically generates adaptations. Users can use simple sliders
      to customize the adaptations to better suit their particular needs and
      preferences, such as increasing the tightness for gripping, enhancing
      torque for rotation, or making a larger base for stability. Finally,
      Reprise provides a toolkit of fastening methods and support structures for
      fitting the adaptations onto existing objects.
    bibtex: >-
      @inproceedings{chen2016reprise,<br>&nbsp; &nbsp; &nbsp;
      &nbsp;title={Reprise: A Design Tool for Specifying, Generating, and
      Customizing 3D Printable Adaptations on Everyday Objects},<br>&nbsp;
      &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Kim, Jeeeun and
      Mankoff, Jennifer and Grossman, Tovi and Coros, Stelian and Hudson, Scott
      E},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={the 29th Annual ACM Symposium
      on User Interface Software and Technology},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2016},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}
    album: '72157674285753686'
    thumbnail: uist2016_reprise_thumbnail.jpg
    paperUrl: 'https://www.dropbox.com/s/j8kywu7dex12ery/uist2016_reprise.pdf'
    citation: >-
      Chen, X., Kim, J., Mankoff, J., Grossman, T., Coros, S., & Hudson, S. E. (2016). Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects. In the 29th Annual ACM Symposium on User Interface Software and Technology.
  - img: t_locus.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Yang Li (Google)"
    name: Locus
    pubs: UIST 2016
    descp: >-
      Bootstrapping user-defined body tapping recognition with offline-learned
      probabilistic representation.
    title: >-
      Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned
      Probabilistic Representation
    abstract: >-
      To address the increasing functionality (or information) overload of
      smartphones, prior research has explored a variety of methods to extend
      the input vocabulary of mobile devices. In particular, body tapping has
      been previously proposed as a technique that allows the user to quickly
      access a target functionality by simply tapping at a specific location of
      the body with a smartphone. Though compelling, prior work often fell short
      in enabling users' unconstrained tapping locations or behaviors. To
      address this problem, we developed a novel recognition method that
      combines both offline—before the system sees any user-defined gestures—and
      online learning to reliably recognize arbitrary, user-defined body tapping
      gestures, only using a smartphone's built-in sensors. Our experiment
      indicates that our method significantly outperforms baseline approaches in
      several usage conditions. In particular, provided only with a single
      sample per location, our accuracy is 30.8% over an SVM baseline and 24.8%
      over a template matching method. Based on these findings, we discuss how
      our approach can be generalized to other user-defined gesture problems. 
    bibtex: >-
      @inproceedings{chen2016bootstrapping,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Li, Yang},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 29th Annual Symposium on User Interface Software and Technology},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={359--364},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2016},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}
    pubs: UIST '16
    video: '184405883'
    videoSite: vimeo
    album: '72157671061225703'
    thumbnail: uist2016_locus_thumbnail.jpg
    paperUrl: 'https://www.dropbox.com/s/iw4eqv7b9rhzykx/uist2016_locus.pdf'
    citation: >-
      Chen, X., & Li, Y. (2016). Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (pp. 359-364).
  - img: t_encore.jpg
    name: Encore
    pubs: UIST 2015
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Stelian Coros (ETH Zurich)"
      - "Jennifer Mankoff (University of Washington)"
      - "Scott Hudson (Carnegie Mellon University)"
    descp: >-
      3D printed augmentation of everyday objects with printed-over, affixed and
      interlocked attachment
    title: >-
      Encore: 3D Printed Augmentation of Everyday Objects with Printed-Over,
      Affixed and Interlocked Attachment
    pubs: UIST '15
    video: '135636261'
    videoSite: vimeo
    abstract: >-
      One powerful aspect of 3D printing is its ability to extend, repair, or
      more generally modify everyday objects. However, nearly all existing work
      implicitly assumes that whole objects are to be printed from scratch. This
      paper presents a framework for 3D printing to augment existing objects
      that covers a wide range of attachment options. We illustrate the
      framework through three exemplar attachment techniques - print-over,
      print-toaffix and print-through, implemented in Encore, a design tool that
      supports a set of analysis metrics relating to viability, durability and
      usability that are visualized for the user to explore design options and
      tradeoffs. Encore also generates 3D models for production, addressing
      issues such as support jigs and contact geometry between the attached part
      and the original object. Our validation helps to illustrate the strengths
      and weaknesses of each technique. For example, we characterize how surface
      curvature and roughness affect print-over's strength compared to the
      conventional print-in-one-piece. 
    bibtex: >-
      @inproceedings{chen2015encore,<br>&nbsp; &nbsp;title={Encore: 3D printed
      augmentation of everyday objects with printed-over, affixed and
      interlocked attachments},<br>&nbsp; &nbsp;author={Chen, Xiang 'Anthony' and
      Coros, Stelian and Mankoff, Jennifer and Hudson, Scott E},<br>&nbsp;
      &nbsp;booktitle={Proceedings of the 28th Annual ACM Symposium on User
      Interface Software and Technology},<br>&nbsp;
      &nbsp;pages={73--82},<br>&nbsp; &nbsp;year={2015},<br>&nbsp;
      &nbsp;organization={ACM}<br>}
    album: '72157670690945024'
    thumbnail: uist2015_encore_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1q9lt5EsboF7WBUfFrmlNCZd6SgsbaDkJ/view?usp=sharing
    citation: >-
      Chen, X., Coros, S., Mankoff, J., & Hudson, S. E. (2015). Encore: 3D printed augmentation of everyday objects with printed-over, affixed and interlocked attachments. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (pp. 73-82).
  
  - img: t_duet.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Tovi Grossman (University of Toronto)"
      - "Daniel Wigdor (University of Toronto)"
      - "George Fitzmaurice (Autodesk Research)"
    name: Duet
    pubs: CHI 2014 🏆
    descp: Joint interaction between a smart phone and a smart watch.
    title: 'Duet: Joint Interaction Between a Smart Phone and a Smart Watch'
    abstract: >-
      The emergence of smart devices (e.g., smart watches and smart eyewear) is
      redefining mobile interaction from the solo performance of a smart phone,
      to a symphony of multiple devices. In this paper, we present Duet - an
      interactive system that explores a design space of interactions between a
      smart phone and a smart watch. Based on the devices' spatial
      configurations, Duet coordinates their motion and touch input, and extends
      their visual and tactile output to one another. This transforms the watch
      into an active element that enhances a wide range of phone-based
      interactive tasks, and enables a new class of multi-device gestures and
      sensing techniques. A technical evaluation shows the accuracy of these
      gestures and sensing techniques, and a subjective study on Duet provides
      insights, observations, and guidance for future work. 
    bibtex: >-
      @inproceedings{chen2014duet,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Duet:
      exploring joint interactions on a smart phone and a smart
      watch},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and
      Grossman, Tovi and Wigdor, Daniel J and Fitzmaurice, George},<br>&nbsp;
      &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the SIGCHI Conference on
      Human Factors in Computing Systems},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;pages={159--168},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2014},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}
    video: '81358039'
    videoSite: vimeo
    album: '72157672834628310'
    thumbnail: chi2014_duet_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1qKdbguC9hnpShEfLEecFOo2V24hWkNlU/view?usp=sharing
    citation: >-
      Chen, X., Grossman, T., Wigdor, D. J., & Fitzmaurice, G. (2014). Duet: exploring joint interactions on a smart phone and a smart watch. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 159-168).
  
  - img: t_airtouch.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Julia Schwarz, Microsoft Research"
      - "Chris Harrison (Carnegie Mellon University)"
      - "Jennifer Mankoff (University of Washington)"
      - "Scott Hudson (Carnegie Mellon University)"
    name: Air+Touch
    descp: Combining In-Air Gesture and Touch Input Above Mobile Devices.
    title: 'Air + touch: interweaving touch and in-air gestures'
    pubs: UIST 2014
    video: '92972949'
    videoSite: vimeo
    abstract: "We present Air+Touch, a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events. For example, a user can draw a circle in the air and tap to trigger a context menu, do a finger 'high jump' between two touches to select a region of text, or drag and in-air 'pigtail' to copy text to the clipboard. Through an observational study, we devised a basic taxonomy of Air+Touch interactions, based on whether the in-air component occurs before, between or after touches. To illustrate the potential of our approach, we built four applications that showcase seven exemplar Air+Touch interactions we created."
    bibtex: "@inproceedings{chen2014air+,<br>&nbsp; &nbsp;title={Air+ touch: interweaving touch and in-air gestures},<br>&nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Schwarz, Julia and Harrison, Chris and Mankoff, Jennifer and Hudson, Scott E},<br>&nbsp; &nbsp;booktitle={Proceedings of the 27th annual ACM symposium on User interface software and technology},<br>&nbsp; &nbsp;pages={519--525},<br>&nbsp; &nbsp;year={2014},<br>&nbsp; &nbsp;organization={ACM}<br>}"
    album: '72157670702717043'
    thumbnail: uist2014_airtouch_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1tTRUt6ilITNmeIeHUx-SMryzVPlVoV7Q/view?usp=sharing
    citation: >-
      Chen, X., Schwarz, J., Harrison, C., Mankoff, J., & Hudson, S. E. (2014). Air + touch: interweaving touch & in-air gestures. In Proceedings of the 27th annual ACM symposium on User interface software and technology (pp. 519-525).

  - img: t_swipeboard.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Tovi Grossman (University of Toronto)"
      - "George Fitzmaurice (Autodesk Research)"
    name: Swipeboard
    descp: An eyes-free text entry technique for ultra-small interfaces
    title: 'Swipeboard: A Text Entry Technique for Ultra-Small Interfaces That Supports Novice to Expert Transitions'
    abstract: >-
      Ultra-small smart devices, such as smart watches, have become increasingly
      popular in recent years. Most of these devices rely on touch as the
      primary input modality, which makes tasks such as text entry increasingly
      difficult as the devices continue to shrink. In the sole pursuit of entry
      speed, the ultimate solution is a shorthand technique (e.g., Morse code)
      that sequences tokens of input (e.g., key, tap, swipe) into unique
      representations of each character. However, learning such techniques is
      hard, as it often resorts to rote memory. Our technique, Swipeboard,
      leverages our spatial memory of a QWERTY keyboard to learn, and eventually
      master a shorthand, eyes-free text entry method designed for ultra-small
      interfaces. Characters are entered with two swipes; the first swipe
      specifies the region where the character is located, and the second swipe
      specifies the character within that region. Our study showed that with
      less than two hours' training, Tested on a reduced word set, Swipeboard
      users achieved 19.58 words per minute (WPM), 15% faster than an existing
      baseline technique.
    bibtex: >-
      @inproceedings{chen2014swipeboard,<br>&nbsp; &nbsp; &nbsp;
      &nbsp;title={Swipeboard: a text entry technique for ultra-small interfaces
      that supports novice to expert transitions},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;author={Chen, Xiang 'Anthony' and Grossman, Tovi and Fitzmaurice,
      George},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 27th
      annual ACM symposium on User interface software and technology},<br>&nbsp;
      &nbsp; &nbsp; &nbsp;pages={615--620},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2014},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}
    pubs: UIST 2014
    video: '143152207'
    videoSite: vimeo
    album: '72157674290832196'
    thumbnail: uist2014_swipeboard_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1ur2XGPAko-P-dv6dtVPinxM5pAVt_0jL/view?usp=sharing
    citation: >-
      Chen, X., Grossman, T., & Fitzmaurice, G. (2014). Swipeboard: a text entry technique for ultra-small interfaces that supports novice to expert transitions. In Proceedings of the 27th annual ACM symposium on User interface software and technology (pp. 615-620).

  - img: t_aroundbody.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Julia Schwarz (Microsoft Research)"
      - "Chris Harrison (Carnegie Mellon University)"
      - "Jennifer Mankoff (University of Washington)"
      - "Scott Hudson (Carnegie Mellon University)"
    name: Around-Body Interaction
    descp: Tracking a commodity smart phone's position around a user's body.
    pubs: MobileHCI 2014
    title: 'Around-Body Interaction: Sensing & Interaction Techniques for Proprioception-Enhanced Input with Mobile Devices'
    abstract: "The space around the body provides a large interaction vol-ume that can allow for big interactions on small mobile de-vices. However, interaction techniques making use of this opportunity are underexplored, primarily focusing on dis-tributing information in the space around the body. We demonstrate three types of around-body interaction includ-ing canvas, modal and context-aware interactions in six demonstration applications. We also present a sensing solu-tion using standard smartphone hardware: a phone's front camera, accelerometer and inertia measurement units. Our solution allows a person to interact with a mobile device by holding and positioning it between a normal field of view and its vicinity around the body. By leveraging a user's proprioceptive sense, around-body Interaction opens a new input channel that enhances conventional interaction on a mobile device without requiring additional hardware."
    bibtex: "@inproceedings{chen2014around,<br>&nbsp; &nbsp; &nbsp; &nbsp;title={Around-body interaction: sensing and interaction techniques for proprioception-enhanced input with mobile devices},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen, Xiang 'Anthony' and Schwarz, Julia and Harrison, Chris and Mankoff, Jennifer and Hudson, Scott},<br>&nbsp; &nbsp; &nbsp; &nbsp;booktitle={Proceedings of the 16th international conference on Human-computer interaction with mobile devices and services},<br>&nbsp; &nbsp; &nbsp; &nbsp;pages={287--290},<br>&nbsp; &nbsp; &nbsp; &nbsp;year={2014},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}"
    video: '142935370'
    videoSite: vimeo
    album: '72157674813685485'
    thumbnail: mhci2014_aroundbody_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1v-zVmJfszd-61yad712ZGiJE8xnEG75R/view?usp=sharing
    citation: >-
      Chen, X., Schwarz, J., Harrison, C., Mankoff, J., & Hudson, S. (2014). Around-body interaction: sensing & interaction techniques for proprioception-enhanced input with mobile devices. In Proceedings of the 16th international conference on Human-computer interaction with mobile devices & services (pp. 287-290).
  
  - img: t_bci.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Nicolai Marquardt (University College London)"
      - "Anthony Tang (University of Toronto)"
      - "Sebastian Boring (Aalborg University)"
      - "Saul Greenberg (University of Calgary)"
    name: Body-Centric Interaction
    descp: >-
      A series of exploration of interacting in the space on and around our
      body.
    pubs: MobileHCI 2012
    title: >-
      Extending a Mobile Device's Interaction Space through Body-Centric
      Interaction
    abstract: >-
      Modern mobile devices rely on the screen as a primary input modality. Yet
      the small screen real-estate limits interaction possibilities, motivating
      researchers to explore alternate input techniques. Within this arena, our
      goal is to develop Body-Centric Interaction with Mobile Devices: a class
      of input techniques that allow a person to position and orient her mobile
      device to navigate and manipulate digital content anchored in the space on
      and around the body. To achieve this goal, we explore such interaction in
      a bottomup path of prototypes and implementations. From our experiences,
      as well as by examining related work, we discuss and present three
      recurring themes that characterize how these interactions can be realized.
      We illustrate how these themes can inform the design of Body-Centric
      Interactions by applying them to the design of a novel mobile browser
      application. Overall, we contribute a class of mobile input techniques
      where interactions are extended beyond the small screen, and are instead
      driven by a person's movement of the device on and around the body.
    bibtex: >-
      @inproceedings{chen2012extending,<br>&nbsp; &nbsp; &nbsp;
      &nbsp;title={Extending a mobile device's interaction space through
      body-centric interaction},<br>&nbsp; &nbsp; &nbsp; &nbsp;author={Chen,
      Xiang 'Anthony' and Marquardt, Nicolai and Tang, Anthony and Boring,
      Sebastian and Greenberg, Saul},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;booktitle={Proceedings of the 14th international conference on
      Human-computer interaction with mobile devices and services},<br>&nbsp;
      &nbsp; &nbsp; &nbsp;pages={151--160},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2012},<br>&nbsp; &nbsp; &nbsp; &nbsp;organization={ACM}<br>}
    video: '31172179'
    videoSite: vimeo
    flickr: '72157674424983716'
    thumbnail: mhci2012_bci_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1vRUr7JYKnpujl6K1b0mAL8lFRZH0b16p/view?usp=sharing
    citation: >-
      Chen, X., Marquardt, N., Tang, A., Boring, S., & Greenberg, S. (2012). Extending a mobile device's interaction space through body-centric interaction. In Proceedings of the 14th international conference on Human-computer interaction with mobile devices and services (pp. 151-160).
  
  - img: t_spalendar.jpg
    authors:
      - "Xiang 'Anthony' Chen (UCLA HCI Research)"
      - "Sebastian Boring (Aalborg University)"
      - "Sheelagh Carpendale (Simon Fraser University)"
      - "Anthony Tang (University of Toronto)"
      - "Saul Greenberg (University of Calgary)"
    name: Spalendar
    descp: Visualizing calendar data by showing people moving between places.
    pubs: AVI 2012
    title: >-
      Spalendar: Visualizing a Group's Calendar Events over a Geographic Space
      on a Public Display 
    abstract: >-
      Portable paper calendars (i.e., day planners and organizers) have greatly
      influenced the design of group electronic calendars. Both use time units
      (hours/days/weeks/etc.) to organize visuals, with useful information
      (e.g., event types, locations, attendees) usually presented as - perhaps
      abbreviated or even hidden - text fields within those time units. The
      problem is that, for a group, this visual sorting of individual events
      into time buckets conveys only limited information about the social
      network of people. For example, people's whereabouts cannot be read 'at a
      glance' but require examining the text. Our goal is to explore an
      alternate visualization that can reflect and illustrate group members'
      calendar events. Our main idea is to display the group's calendar events
      as spatiotemporal activities occurring over a geographic space animated
      over time, all presented on a highly interactive public display. In
      particular, our SPALENDAR (SPAtial CALENDAR) design animates peoples'
      past, present and forthcoming movements between event locations as well as
      their static locations. Details of people's events, their movements and
      their locations are progressively revealed and controlled by the viewer's
      proximity to the display, their identity, and their gestural interactions
      with it, all of which are tracked by the public display.
    bibtex: >-
      @inproceedings{chen2012spalendar,<br>&nbsp; &nbsp; &nbsp;
      &nbsp;title={Spalendar: visualizing a group's calendar events over a
      geographic space on a public display},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;author={Chen, Xiang 'Anthony' and Boring, Sebastian and Carpendale,
      Sheelagh and Tang, Anthony and Greenberg, Saul},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;booktitle={Proceedings of the International Working Conference on
      Advanced Visual Interfaces},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;pages={689--696},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;year={2012},<br>&nbsp; &nbsp; &nbsp;
      &nbsp;organization={ACM}<br>}<br>
    video: '31823922'
    videoSite: vimeo
    thumbnail: avi2012_spalendar_thumbnail.jpg
    paperUrl: https://drive.google.com/file/d/1wUixLiuba4Faue9iAqwkZJa9JPiWxrz2/view?usp=sharing
    citation: >-
      Chen, X., Boring, S., Carpendale, S., Tang, A., & Greenberg, S. (2012). Spalendar: visualizing a group's calendar events over a geographic space on a public display. In AVI '12 Proceedings of the International Working Conference on Advanced Visual Interfaces (pp. 689-696).

aboutus:
  - mission: "Our mission is to innovate interactive systems that catalyze advances in AI to achieve three levels of human-centered objectives:<br/><ul>
    <li>Aligning with human values</li>
    <li>Assimilating human thoughts</li>
    <li>Augmenting human abilities</li>
    </ul>"
    photos: 
      # - g_bbq.jpg
      - g_postchi.jpg
      - g-bruce-uist.jpg 
      # - g_collaboration.JPG
      # - g_ruolin_uist19.jpg
      - g-hongyan-ichi.JPG 
      - g-noyan-defense.jpg
    contact:
      description: UCLA HCI Research is part of Electrical & Computer Engineering Department and Computer Science Department in the Henry Samueli School of Engineering at UCLA.
      address: <b>UCLA HCI Research</b><br/>1538 Boelter Hall (UCLA HCI Research)<br/>Los Angeles, CA 90095
      email: Email: sigchi @ ucla.edu
    sponsors:
      - img: onr.png
        url: https://www.nre.navy.mil/education-outreach/sponsored-research/yip/2021-young-investigators
      - img: nsf.png
        url: https://nsf.gov/
      - img: ucla.png
        url: https://grad.ucla.edu/funding/financial-aid/summer-mentored-research-fellowship-smrf/
      - img: hellman.png
        url: https://www.universityofcalifornia.edu/sites/default/files/hellman-fellows-2020.pdf
      - img: salesforce.png
        url: https://www.salesforceairesearch.com/
      - img: google.png
        url: https://research.google/outreach/research-scholar-program/
      - img: adobe.png
        url: https://www.adobe.com/
      - img: amazon.png
        url: https://www.sciencehub.ucla.edu/2023-amazon-fellows/
      - img: intel.png
        url: https://www.intel.com/content/www/us/en/research/blogs/intel-rising-stars-awards-2022.html
      # - img: meta.png
      #   url: https://www.metachnology.com/
    map: <iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d1991.4035161501527!2d-118.44520705236775!3d34.06920012829791!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80c2bc8632b4bcf3%3A0x8087319fe00f1e04!2sUCLA+HCI+Research!5e0!3m2!1sen!2sus!4v1555280319576!5m2!1sen!2sus" width="500" height="400" frameborder="0" style="border:0" allowfullscreen></iframe>
redirects:
  - name: writing
    url: http://67.158.54.10/
  - name: reflection
    url: http://67.158.54.10/exp
  - name: lbr
    url: https://www.notion.so/uclahci/Learning-by-Research-Pragmatic-Educational-Research-Participation-for-Undergrads-148956558a3c4836b003065ea692e9dd
  - name: 209as
    url: https://www.notion.so/uclahci/2020-Fall-ECE-M209AS-Human-Computer-Interaction-4a46bd7275b34bce8e48ba7ad1325d7f
  - name: bib
    url: https://chrome.google.com/webstore/detail/bib-%E2%80%94-copy-bibtex-in-1-cl/onnmdchfagapkggbhnnjkmllimegclnh
  - name: counterweight
    url: https://chrome.google.com/webstore/detail/counterweight/oemlabbngpnemncoplnmcpdlnpgcgiih
